{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntroducciÃ³n\n",
    "En este notebook, exploramos diferentes modelos de procesamiento de lenguaje natural (NLP) para construir un recomendador de canciones basado en letras y estados de Ã¡nimo. Evaluamos varios modelos y finalmente elegimos el que mejor se adapta a nuestras necesidades.\n",
    "\n",
    "**Quisiera pedir perdÃ³n ya que es un notebook recuperado que ha perdido su mayor esencia y mucho del trabajo que llevaba (gajes de ser nueva en el mundo git)**\n",
    "\n",
    "En el cuaderno [modelos_perdido.ipynb](functions/4%20-%20Recomendador%20letras%20canciones/modelos_perdido.ipynb) corrompido, salen algunos de los resultados de los modelos, aunque las mÃ©tricas de algunos eran mayores en similaridad el que mÃ¡s nos ha convencido finalmente ha sido el de Roberta el cual en cuaderno [Berta_Final.ipynb](functions/4%20-%20Recomendador%20letras%20canciones/Berta_Final.ipynb) podrÃ©is ver que lo hemos mezclado con etiqutas FAISS para una bÃºsqueda mÃ¡s rÃ¡pida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ ComparaciÃ³n de Modelos para BÃºsqueda SemÃ¡ntica de Canciones\n",
    "\n",
    "| **Modelo**                  | **Ventajas**                                         | **Desventajas**                                       | **Recomendado?** | **DimensiÃ³n Embeddings** |\n",
    "|-----------------------------|----------------------------------------------------|-----------------------------------------------------|-------------------------|--------------------------|\n",
    "| **âœ… all-roberta-large-v1**  | ðŸ”¹ Muy preciso en textos largos. ðŸ”¹ Captura contexto profundo. | ðŸ”¹ MÃ¡s lento en inferencia. ðŸ”¹ Requiere mÃ¡s RAM/GPU. | âœ… **SÃ­, es el modelo que hemos elegido por su alta precisiÃ³n.** | 1024 |\n",
    "| **âœ… sentence-t5-base**      | ðŸ”¹ Optimizado para bÃºsqueda semÃ¡ntica. ðŸ”¹ Puede generar textos (opcional). | ðŸ”¹ No estÃ¡ entrenado en datos musicales. | âœ… **SÃ­, es nuestra segunda opciÃ³n por su equilibrio entre precisiÃ³n y eficiencia.** | 768 |\n",
    "| **ðŸ”¥ all-mpnet-base-v2**     | ðŸ”¹ Muy equilibrado: rÃ¡pido y preciso. ðŸ”¹ Optimizado para comparaciÃ³n semÃ¡ntica. | ðŸ”¹ No es el mÃ¡s avanzado, pero tiene buen rendimiento. | âš ï¸ **Tal vez, si queremos una opciÃ³n rÃ¡pida y precisa.** | 768 |\n",
    "| **ðŸ”¥ all-mpnet-large**       | ðŸ”¹ Mismo modelo que usamos pero mÃ¡s preciso. ðŸ”¹ Optimizado para comprensiÃ³n profunda. | ðŸ”¹ Puede ser mÃ¡s lento que `mpnet-base-v2`. | âš ï¸ **Roberta Nos gusta mÃ¡s para nuestra finalidad.** | 1024 |\n",
    "| **ðŸ”¥ all-distilroberta-v1**  | ðŸ”¹ MÃ¡s rÃ¡pido que `roberta-large`. ðŸ”¹ Muy bueno en bÃºsqueda semÃ¡ntica. | ðŸ”¹ No es tan profundo como `roberta-large`. | âš ï¸ **Tal vez, si necesitamos mÃ¡s velocidad sin perder mucha precisiÃ³n.** | 768 |\n",
    "| **Bert**                    | ðŸ”¹ Bien entrenado en datos generales. ðŸ”¹ Buen rendimiento en tareas generales de NLP. | ðŸ”¹ No tan optimizado para datos musicales especÃ­ficos. | âŒ **No, preferimos modelos mÃ¡s especializados.** | 768 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento de Red Neuronal para Juntar Embeddings de T5, RoBERTa y MPNet\n",
    "Se intentÃ³ utilizar una red neuronal para combinar los embeddings generados por los modelos T5, RoBERTa y MPNet con el objetivo de mejorar la precisiÃ³n de las recomendaciones. Sin embargo, este enfoque no resultÃ³ ser eficiente en tÃ©rminos de tiempo de procesamiento y rendimiento. Por lo tanto, se decidiÃ³ no utilizar esta tÃ©cnica en la implementaciÃ³n final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarte del Clustering y Formateo Total del Texto\n",
    "\n",
    "### Clustering\n",
    "Inicialmente, consideramos utilizar tÃ©cnicas de clustering para agrupar canciones con letras similares. Sin embargo, este enfoque fue descartado por varias razones:\n",
    "\n",
    "1. **Complejidad Computacional:** El clustering de grandes volÃºmenes de datos textuales requiere una cantidad significativa de recursos computacionales, lo que puede ser ineficiente y lento.\n",
    "2. **PÃ©rdida de InformaciÃ³n SemÃ¡ntica:** Los mÃ©todos de clustering tradicionales pueden no capturar adecuadamente la riqueza semÃ¡ntica y el contexto de las letras de las canciones, lo que resulta en agrupaciones menos precisas.\n",
    "3. **Dificultad en la InterpretaciÃ³n:** Los resultados del clustering pueden ser difÃ­ciles de interpretar y explicar, especialmente cuando se trata de datos textuales complejos como las letras de canciones.\n",
    "\n",
    "### Formateo Total del Texto\n",
    "TambiÃ©n se probÃ³ a realizar un formateo exhaustivo de las letras de las canciones para normalizar el texto. Sin embargo, este enfoque fue descartado por las siguientes razones:\n",
    "\n",
    "1. **PÃ©rdida de Contexto:** El formateo excesivo puede eliminar informaciÃ³n contextual importante, como la estructura poÃ©tica y las expresiones idiomÃ¡ticas, que son cruciales para la comprensiÃ³n semÃ¡ntica.\n",
    "2. **Complejidad del Preprocesamiento:** Implementar un formateo total del texto requiere un preprocesamiento complejo y detallado, lo que puede ser propenso a errores y difÃ­cil de mantener.\n",
    "3. **Impacto Negativo en el Rendimiento del Modelo:** La normalizaciÃ³n excesiva del texto puede afectar negativamente el rendimiento de los modelos de lenguaje, que estÃ¡n diseÃ±ados para manejar variaciones en el texto natural.\n",
    "\n",
    "En lugar de estos enfoques, optamos por utilizar modelos de lenguaje avanzados como **all-roberta-large-v1** y **sentence-t5-base**, que pueden capturar de manera mÃ¡s efectiva la semÃ¡ntica y el contexto de las letras de las canciones sin necesidad de un preprocesamiento excesivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar LibrerÃ­as\n",
    "Importamos las librerÃ­as necesarias para el anÃ¡lisis y procesamiento de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Debe devolver True si hay GPU disponible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el Dataset\n",
    "Cargamos el archivo CSV que contiene las letras de las canciones y otros metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV\n",
    "file_path = r'C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\final_df.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InformaciÃ³n del Dataset\n",
    "Mostramos informaciÃ³n general del dataset, incluyendo el nÃºmero de entradas y las columnas disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Datos\n",
    "Realizamos una limpieza bÃ¡sica de los datos, eliminando duplicados y manejando valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Manejar valores nulos\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de Texto\n",
    "Definimos funciones para limpiar y formatear las letras de las canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Convertir a minÃºsculas\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplazar mÃºltiples espacios con uno solo\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Eliminar caracteres especiales\n",
    "    text = text.strip()  # Eliminar espacios al inicio y final\n",
    "    return text\n",
    "\n",
    "def format_text_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_format = ['song_name', 'artist_name', 'album_name', 'playlists_names', 'combined_genres', 'processed_lyrics']\n",
    "df = format_text_columns(df, columns_to_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExploraciÃ³n de Datos\n",
    "Realizamos una exploraciÃ³n inicial de los datos para entender mejor su estructura y contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          artist_name                song_name  \\\n",
    "91   birds of chicago         estrella goodbye   \n",
    "109         black rob              muscle game   \n",
    "134      balsam range          trains i missed   \n",
    "135            lights     cactus in the valley   \n",
    "167    ray lamontagne         change your mind   \n",
    "181     steve winwood           theres a river   \n",
    "190      the vaccines           young american   \n",
    "280         alpha 520     les larmes du soleil   \n",
    "300             loote   high without your love   \n",
    "375        tom t hall  legend of the lady bear   \n",
    "\n",
    "                                               spotify_url  \\\n",
    "91   https://open.spotify.com/track/5jcetnvnAWGTMGufJ7oEnc   \n",
    "109  https://open.spotify.com/track/5xYb779c1wmOmbfaexX8JK   \n",
    "134  https://open.spotify.com/track/1ap81IwnNa5QeuvMMnbZfu   \n",
    "135  https://open.spotify.com/track/288xqEJiCcSc2zGFanlclV   \n",
    "167  https://open.spotify.com/track/66r7ecZd8LGtJWb9t6PhA6   \n",
    "181  https://open.spotify.com/track/6IbNMlVEr2GY2DpkxINOts   \n",
    "190  https://open.spotify.com/track/4gcZVTYjOBMh1UVlZy2YJW   \n",
    "280  https://open.spotify.com/track/3ItQEhTWSvMOHTinqdxGB0   \n",
    "300  https://open.spotify.com/track/3hwyRkhcVkcgUJHKKmxS8k   \n",
    "375  https://open.spotify.com/track/4jWcTUjobvn6pzgN3b7w7l  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InformaciÃ³n del Dataset\n",
    "Mostramos informaciÃ³n general del dataset, incluyendo el nÃºmero de entradas y las columnas disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Convertir a minÃºsculas\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplazar mÃºltiples espacios con uno solo\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Eliminar caracteres especiales\n",
    "    text = text.strip()  # Eliminar espacios al inicio y final\n",
    "    return text\n",
    "\n",
    "def format_text_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_format = ['song_name', 'artist_name', 'album_name', 'playlists_names', 'combined_genres', 'processed_lyrics']\n",
    "df = format_text_columns(df, columns_to_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nulos\n",
    "pd.set_option('display.max_rows', None) \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 30) \n",
    "# visualizar contenido de la celda spotify_url entero cuando se filtra en pandas\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar canciones con \"mood_sad\" alto y palabras clave en las letras\n",
    "keywords = [\"playa\", \"mar\", \"ocÃ©ano\", \"olas\", \"arena\", \"brisa\", \"agua\"]\n",
    "filtered_songs = df[(df['mood_sad'] > 0.7) & df['processed_lyrics'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "\n",
    "# Mostrar las primeras 10 filas\n",
    "print(filtered_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRUEBAS DE MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# all-MiniLM-L6-v2 - 384 Emb\n",
    "\n",
    "**DescripciÃ³n del Modelo:**\n",
    "El modelo **all-MiniLM-L6-v2** es una versiÃ³n ligera y eficiente de los modelos de lenguaje. EstÃ¡ diseÃ±ado para generar embeddings de alta calidad con una dimensiÃ³n de 384. Este modelo es conocido por su rapidez y eficiencia en el uso de recursos, lo que lo hace ideal para aplicaciones en tiempo real y sistemas con limitaciones de memoria y procesamiento.\n",
    "\n",
    "**Ventajas:**\n",
    "- **RÃ¡pido y Ligero:** Debido a su menor tamaÃ±o, el modelo es extremadamente rÃ¡pido en la generaciÃ³n de embeddings y consume menos memoria RAM y GPU.\n",
    "- **Eficiente:** Ideal para aplicaciones que requieren respuestas rÃ¡pidas y en tiempo real, como chatbots y sistemas de bÃºsqueda rÃ¡pida.\n",
    "\n",
    "**Desventajas:**\n",
    "- **Menor PrecisiÃ³n:** Aunque es rÃ¡pido y eficiente, la precisiÃ³n de los embeddings generados por **all-MiniLM-L6-v2** es menor en comparaciÃ³n con modelos mÃ¡s grandes y complejos.\n",
    "- **Menor Capacidad de ComprensiÃ³n:** No captura el contexto y la semÃ¡ntica de los textos tan bien como otros modelos mÃ¡s avanzados.\n",
    "\n",
    "**RazÃ³n para No Usarlo en el Proyecto:**\n",
    "Para este proyecto, que implica la recomendaciÃ³n de canciones basadas en letras y estados de Ã¡nimo o frases, se requiere un modelo que pueda capturar de manera precisa y profunda el contexto y la semÃ¡ntica de las letras de las canciones. Aunque **all-MiniLM-L6-v2** es rÃ¡pido y eficiente, su menor precisiÃ³n y capacidad de comprensiÃ³n no son suficientes para las necesidades del proyecto. Modelos como **all-roberta-large-v1** y **sentence-t5-base** ofrecen una mejor comprensiÃ³n semÃ¡ntica y precisiÃ³n, lo que los hace mÃ¡s adecuados para este tipo de tareas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ“Œ Cargar modelo y GPU \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# ðŸ“Œ Cargar dataset con embeddings\n",
    "embeddings_file = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings.pkl\"\n",
    "df = pd.read_pickle(embeddings_file)\n",
    "\n",
    "# ðŸ“Œ Convertir embeddings a arrays NumPy para mejorar rendimiento\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Detecta el idioma y traduce al inglÃ©s si es necesario.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':  # Si el texto no estÃ¡ en inglÃ©s, traducimos\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"ðŸŒ Traducido '{text}' âž '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text  # Si ya estÃ¡ en inglÃ©s, no hacemos nada\n",
    "\n",
    "def translate_to_spanish(text):\n",
    "    \"\"\"Traduce la letra de la canciÃ³n al espaÃ±ol.\"\"\"\n",
    "    if pd.notna(text):  # Solo traducimos si hay letra disponible\n",
    "        return GoogleTranslator(source='en', target='es').translate(text)\n",
    "    return \"TraducciÃ³n no disponible\"\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    BÃºsqueda semÃ¡ntica de canciones con traducciÃ³n automÃ¡tica si el usuario escribe en otro idioma.\n",
    "    \"\"\"\n",
    "    # ðŸ“Œ Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # ðŸ“Œ Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "    # ðŸ“Œ Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity(x.reshape(1, -1), query_embedding)[0][0])\n",
    "\n",
    "    # ðŸ“Œ Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n).copy()\n",
    "\n",
    "    # ðŸ“Œ Traducir las letras de las canciones al espaÃ±ol\n",
    "    top_songs['translated_lyrics'] = top_songs['processed_lyrics'].apply(lambda x: translate_to_spanish(x[:500]) if isinstance(x, str) else \"\")\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'translated_lyrics', 'similarity']]\n",
    "\n",
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"Canciones para bailar con mis amigas\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*80, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"me encanta viajar sola\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*80, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"sin mÃºsica la vida serÃ­a un error\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*80, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"He cometido el peor de los pecados que un hombre puede cometer: no he sido feliz\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all-mpnet-base-v2 - 768 Emb\n",
    "\n",
    "**DescripciÃ³n del Modelo:**\n",
    "El modelo **all-mpnet-base-v2** es conocido por su equilibrio entre precisiÃ³n y eficiencia. Genera embeddings de alta calidad con una dimensiÃ³n de 768, lo que lo hace adecuado para una variedad de tareas de procesamiento de lenguaje natural, incluyendo la bÃºsqueda semÃ¡ntica y la comparaciÃ³n de textos.\n",
    "\n",
    "**Ventajas:**\n",
    "- **Alta PrecisiÃ³n:** Ofrece una excelente precisiÃ³n en la comprensiÃ³n semÃ¡ntica de textos.\n",
    "- **Eficiencia:** Es mÃ¡s rÃ¡pido y consume menos recursos que modelos mÃ¡s grandes como **all-roberta-large-v1**.\n",
    "- **Versatilidad:** Adecuado para tareas de bÃºsqueda semÃ¡ntica y comparaciÃ³n de textos.\n",
    "\n",
    "**Desventajas:**\n",
    "- **Menor Capacidad de ComprensiÃ³n:** Aunque es preciso, no captura el contexto y la semÃ¡ntica tan profundamente como modelos mÃ¡s grandes.\n",
    "- **Limitaciones en Textos Largos:** Puede no ser tan efectivo en la comprensiÃ³n de textos muy largos y detallados.\n",
    "\n",
    "**RazÃ³n para No Usarlo en el Proyecto:**\n",
    "Para este proyecto, que implica la recomendaciÃ³n de canciones basadas en letras y estados de Ã¡nimo, se requiere un modelo que pueda capturar de manera precisa y profunda el contexto y la semÃ¡ntica de las letras de las canciones. Aunque **all-mpnet-base-v2** funciona dando unas similitudes mÃ¡s alta, su capacidad de comprensiÃ³n semÃ¡ntica es menor en comparaciÃ³n con modelos mÃ¡s avanzados como **all-roberta-large-v1** y **sentence-t5-base**. Estos modelos ofrecen una mejor comprensiÃ³n semÃ¡ntica y precisiÃ³n, lo que los hace mÃ¡s adecuados para las necesidades del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# ðŸ“Œ Ruta del dataset con los embeddings generados\n",
    "embeddings_file_768 = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_768.pkl\"\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset con embeddings\n",
    "df = pd.read_pickle(embeddings_file_768)\n",
    "\n",
    "# ðŸ“Œ Convertir embeddings a NumPy arrays para mejorar rendimiento\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "# ðŸ“Œ Cargar el modelo (mismo que usamos para generar embeddings)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âš¡ Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para traducir la frase del usuario si no estÃ¡ en inglÃ©s\n",
    "def translate_to_english(text):\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':  \n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"ðŸŒ Traducido '{text}' âž '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text  \n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para traducir las letras de las canciones al espaÃ±ol\n",
    "def translate_to_spanish(text):\n",
    "    if pd.notna(text):  # Solo traducimos si hay letra disponible\n",
    "        return GoogleTranslator(source='en', target='es').translate(text)\n",
    "    return \"TraducciÃ³n no disponible\"\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para buscar canciones segÃºn una frase de usuario\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    Encuentra las mejores coincidencias en el dataset segÃºn la frase ingresada por el usuario.\n",
    "    - user_query: Texto del usuario.\n",
    "    - top_n: NÃºmero de canciones recomendadas.\n",
    "    \"\"\"\n",
    "    # ðŸ“Œ Traducir la frase si no estÃ¡ en inglÃ©s\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # ðŸ“Œ Convertir la frase en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # ðŸ“Œ Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([x], [query_embedding])[0][0])\n",
    "\n",
    "    # ðŸ“Œ Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n).copy()\n",
    "\n",
    "    # ðŸ“Œ Traducir las letras de las canciones al espaÃ±ol\n",
    "    top_songs['translated_lyrics'] = top_songs['processed_lyrics'].apply(\n",
    "        lambda x: translate_to_spanish(x[:500]) if isinstance(x, str) else \"TraducciÃ³n no disponible\"\n",
    "    )\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'translated_lyrics', 'similarity']]\n",
    "\n",
    "# ðŸ” Prueba con una frase del usuario\n",
    "user_input = \"canciones para bailar con mis amigas\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:500], \"...\")\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics)\n",
    "    print(\"=\"*100, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una frase del usuario\n",
    "user_input = \"Me encanta viajar sola\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:500], \"...\")\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics)\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"sin mÃºsica la vida serÃ­a un error\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"He cometido el peor de los pecados que un hombre puede cometer: no he sido feliz\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-t5-base\n",
    "\n",
    "**DescripciÃ³n del Modelo:**\n",
    "El modelo **sentence-t5-base** es conocido por su capacidad para comprender y generar texto de manera eficiente. EstÃ¡ optimizado para tareas de bÃºsqueda semÃ¡ntica y generaciÃ³n de texto, lo que lo hace muy versÃ¡til en aplicaciones de procesamiento de lenguaje natural.\n",
    "\n",
    "**Ventajas:**\n",
    "- **ComprensiÃ³n SemÃ¡ntica:** Excelente en la comprensiÃ³n del significado completo de las frases, no solo de palabras individuales.\n",
    "- **GeneraciÃ³n de Texto:** Puede generar texto coherente y relevante, lo que es Ãºtil para tareas que requieren respuestas generativas.\n",
    "- **Equilibrio:** Ofrece un buen balance entre precisiÃ³n y eficiencia, siendo mÃ¡s rÃ¡pido que modelos mÃ¡s grandes.\n",
    "\n",
    "**Desventajas:**\n",
    "- **No EspecÃ­fico para MÃºsica:** Aunque es potente en comprensiÃ³n semÃ¡ntica, no estÃ¡ especÃ­ficamente entrenado en datos musicales, lo que puede limitar su rendimiento en tareas muy especializadas.\n",
    "- **Menor PrecisiÃ³n en Conceptos Abstractos:** Aunque es bueno en comprensiÃ³n semÃ¡ntica, puede no captar tan bien los conceptos abstractos y matices como otros modelos mÃ¡s avanzados.\n",
    "\n",
    "**RazÃ³n para No Usarlo:**\n",
    "Aunque **sentence-t5-base** fue nuestra segunda opciÃ³n debido a su capacidad para comprender el significado completo de las frases, finalmente optamos por **all-roberta-large-v1**. Los resultados obtenidos con **all-roberta-large-v1** fueron significativamente mejores en la captura de conceptos abstractos y matices en las letras de las canciones. La capacidad de **all-roberta-large-v1** para entender el contexto profundo y los detalles de las letras nos convenciÃ³ de que era la mejor opciÃ³n para nuestro recomendador de canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# ðŸ“Œ Ruta del dataset y archivo de embeddings\n",
    "file_path = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df.csv\"\n",
    "embeddings_file_t5 = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_t5.pkl\"\n",
    "\n",
    "# ðŸ“Œ Cargar dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ðŸ“Œ Crear la columna 'embedding' si no existe\n",
    "if 'embedding' not in df.columns:\n",
    "    df['embedding'] = None  # Evita errores al asignar embeddings mÃ¡s tarde\n",
    "\n",
    "# ðŸ“Œ Usar GPU si estÃ¡ disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âš¡ Usando: {device.upper()}\")  \n",
    "model = SentenceTransformer('sentence-t5-base', device=device)  # ðŸ“Œ Modelo con mÃ¡xima calidad semÃ¡ntica\n",
    "\n",
    "# ðŸ“Œ Crear texto combinado para embeddings\n",
    "df['combined_text'] = (\n",
    "    df['song_name'].astype(str) + \" \" +\n",
    "    df['artist_name'].astype(str) + \" \" +\n",
    "    df['processed_lyrics'].astype(str) + \" \" +\n",
    "    df['combined_genres'].astype(str) + \" \" +\n",
    "    df['playlists_names'].astype(str) + \" \" +\n",
    "    df['album_release_date'].astype(str) + \" \" +\n",
    "    df['language'].astype(str)\n",
    ")\n",
    "\n",
    "# ðŸ“Œ ParÃ¡metros de procesamiento en bloques\n",
    "batch_size = 10000  \n",
    "total_rows = len(df)\n",
    "\n",
    "print(f\"ðŸš€ Total de canciones en el dataset: {total_rows}\")\n",
    "print(f\"ðŸ“Œ Procesando en bloques de {batch_size} canciones por batch.\")\n",
    "\n",
    "# ðŸ“Œ Generar embeddings en bloques de 10,000 canciones\n",
    "for start in range(0, total_rows, batch_size):\n",
    "    end = min(start + batch_size, total_rows)\n",
    "    start_time = time.time()  # â³ Iniciar contador de tiempo\n",
    "    print(f\"\\nâš¡ [Batch {start}-{end}] Iniciando procesamiento...\")\n",
    "\n",
    "    # ðŸ“Œ Obtener lista de textos\n",
    "    text_batch = df.iloc[start:end]['combined_text'].tolist()\n",
    "\n",
    "    # ðŸ“Œ Evitar procesar si el lote estÃ¡ vacÃ­o\n",
    "    if len(text_batch) == 0:\n",
    "        print(f\"âŒ [Batch {start}-{end}] No hay datos para procesar. Saltando...\")\n",
    "        continue  \n",
    "\n",
    "    print(f\"â³ [Batch {start}-{end}] Generando embeddings en GPU...\")\n",
    "\n",
    "    try:\n",
    "        embeddings_batch = model.encode(\n",
    "            text_batch,\n",
    "            batch_size=32,  # ðŸ”¥ MÃ¡xima calidad sin sobrecargar GPU\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en batch {start}-{end}: {str(e)}\")\n",
    "        continue  \n",
    "\n",
    "    print(f\"âœ… [Batch {start}-{end}] Embeddings generados. Guardando...\")\n",
    "\n",
    "    # ðŸ“Œ Convertir cada embedding a lista\n",
    "    embeddings_batch_list = [emb.tolist() for emb in embeddings_batch]  \n",
    "    num_rows = end - start  # NÃºmero de filas en este batch\n",
    "\n",
    "    # ðŸ“Œ Ajustar el tamaÃ±o de la lista si hay desajuste con el nÃºmero de filas\n",
    "    if len(embeddings_batch_list) > num_rows:\n",
    "        embeddings_batch_list = embeddings_batch_list[:num_rows]  \n",
    "    elif len(embeddings_batch_list) < num_rows:\n",
    "        embeddings_batch_list += [[None] * len(embeddings_batch_list[0])] * (num_rows - len(embeddings_batch_list))\n",
    "\n",
    "    # ðŸ“Œ Asignar los embeddings al DataFrame usando iloc y pd.Series\n",
    "    df.iloc[start:end, df.columns.get_loc('embedding')] = pd.Series(embeddings_batch_list, index=df.index[start:end])\n",
    "\n",
    "    # ðŸ“Œ Guardar en archivo para evitar pÃ©rdida de progreso\n",
    "    df.to_pickle(embeddings_file_t5)\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # â± Tiempo tomado\n",
    "    print(f\"âœ… [Batch {start}-{end}] Guardado parcial exitoso en {elapsed_time:.2f} segundos.\\n\")\n",
    "\n",
    "    # ðŸ“Œ PequeÃ±a pausa para evitar sobrecarga de GPU\n",
    "    time.sleep(2)  \n",
    "\n",
    "print(\"ðŸš€ Todos los embeddings generados con el nuevo modelo y guardados correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# ðŸ“Œ Ruta del archivo de embeddings\n",
    "embeddings_file_t5 = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_t5.pkl\"\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset con los embeddings\n",
    "df = pd.read_pickle(embeddings_file_t5)\n",
    "\n",
    "# ðŸ“Œ Cargar modelo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âš¡ Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('sentence-t5-base', device=device)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Traduce la frase del usuario a inglÃ©s si no estÃ¡ en inglÃ©s.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"ðŸŒ Traducido '{text}' âž '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    Busca canciones en base a una frase de usuario.\n",
    "    \n",
    "    - user_query: Texto ingresado por el usuario (ejemplo: \"Me gusta conducir en invierno\").\n",
    "    - top_n: NÃºmero de canciones recomendadas.\n",
    "    \"\"\"\n",
    "    # ðŸ“Œ Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # ðŸ“Œ Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # ðŸ“Œ Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([x], [query_embedding])[0][0])\n",
    "\n",
    "    # ðŸ“Œ Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'similarity']]\n",
    "\n",
    "# ðŸ” Prueba con una consulta\n",
    "user_input = \"mis amigos y amigas son los mejores\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados\n",
    "print(resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset con los embeddings\n",
    "embeddings_file_t5 = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_t5.pkl\"\n",
    "df = pd.read_pickle(embeddings_file_t5)\n",
    "\n",
    "# ðŸ“Œ Convertir embeddings a array NumPy (MUY IMPORTANTE PARA RENDIMIENTO)\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "embedding_matrix = np.vstack(df['embedding'].values)  # Convierte todos los embeddings en un array\n",
    "\n",
    "# ðŸ“Œ Cargar modelo en GPU si estÃ¡ disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âš¡ Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('sentence-t5-base', device=device)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Traduce la frase del usuario a inglÃ©s si no estÃ¡ en inglÃ©s.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"ðŸŒ Traducido '{text}' âž '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "def translate_to_spanish(text):\n",
    "    \"\"\"Traduce la letra de la canciÃ³n al espaÃ±ol si estÃ¡ en inglÃ©s.\"\"\"\n",
    "    if pd.notna(text):  # Solo traducimos si hay letra disponible\n",
    "        return GoogleTranslator(source='en', target='es').translate(text)\n",
    "    return \"TraducciÃ³n no disponible\"\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"Busca canciones en base a una frase del usuario, con cÃ¡lculos optimizados.\"\"\"\n",
    "    \n",
    "    # ðŸ“Œ Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # ðŸ“Œ Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "    # ðŸ“Œ Calcular similitud en un solo paso (MUCHO MÃS RÃPIDO)\n",
    "    similarities = cosine_similarity(query_embedding, embedding_matrix)[0]\n",
    "\n",
    "    # ðŸ“Œ Agregar resultados a un nuevo dataframe\n",
    "    df['similarity'] = similarities\n",
    "\n",
    "    # ðŸ“Œ Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "\n",
    "    # ðŸ“Œ Traducir las letras al espaÃ±ol y agregar columna\n",
    "    top_songs['translated_lyrics'] = top_songs['processed_lyrics'].apply(translate_to_spanish)\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'translated_lyrics', 'similarity']]\n",
    "\n",
    "# ðŸ” PRUEBA de velocidad con una consulta\n",
    "user_input = \"relojes derretidos en el tiemp\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original (Primeros 800 caracteres):**\\n\", lyrics[:800], \"...\")\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola (Primeros 800 caracteres):**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” PRUEBA de velocidad con una consulta\n",
    "user_input = \"Me encanta viajar sola\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original (Primeros 800 caracteres):**\\n\", lyrics[:800], \"...\")\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola (Primeros 800 caracteres):**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"sin mÃºsica la vida serÃ­a un error\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"He cometido el peor de los pecados que un hombre puede cometer: no he sido feliz\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElecciÃ³n del Modelo Final\n",
    "DespuÃ©s de evaluar varios modelos, decidimos quedarnos con **all-roberta-large-v1** debido a su alta capacidad de comprensiÃ³n semÃ¡ntica y su rendimiento en textos largos y detallados. Aunque es mÃ¡s pesado y lento que otros modelos, su precisiÃ³n y capacidad para capturar el contexto profundo de las letras de canciones lo hacen ideal para nuestro recomendador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConclusiÃ³n\n",
    "El modelo **all-roberta-large-v1** ha demostrado ser el mÃ¡s adecuado para nuestro sistema de recomendaciÃ³n de canciones basado en letras y estados de Ã¡nimo. Su capacidad para entender el contexto y la semÃ¡ntica de las frases completas nos permite ofrecer recomendaciones precisas y relevantes a los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImplementaciÃ³n del Modelo Final\n",
    "En esta secciÃ³n, implementamos el modelo **all-roberta-large-v1** para generar embeddings de las letras de canciones y realizar bÃºsquedas semÃ¡nticas eficientes utilizando FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ðŸ“Œ Rutas de archivos\n",
    "file_path = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\cleaned_songs_data_v2.csv\"\n",
    "faiss_index_path = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\lyrics_embeddings_faiss_IP.index\"\n",
    "\n",
    "# ðŸ“Œ Cargar dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ðŸ“Œ Usar GPU si estÃ¡ disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âš¡ Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('sentence-transformers/all-roberta-large-v1', device=device)  # ðŸ”¥ MÃ¡xima calidad\n",
    "\n",
    "# ðŸ“Œ Crear texto combinado para embeddings (usamos `processed_lyrics`)\n",
    "df['combined_text'] = (\n",
    "    df['song_name'].astype(str) + \" \" +\n",
    "    df['artist_name'].astype(str) + \" \" +\n",
    "    df['processed_lyrics'].astype(str) + \" \" +\n",
    "    df['combined_genres'].astype(str) + \" \" +\n",
    "    df['playlists_names'].astype(str) + \" \" +\n",
    "    df['album_release_date'].astype(str) + \" \" +\n",
    "    df['language'].astype(str)\n",
    ")\n",
    "\n",
    "# ðŸ“Œ ParÃ¡metros optimizados para evitar sobrecalentamiento\n",
    "batch_size = 2000  # ðŸ”¹ Reducido de 5000 a 2000\n",
    "pause_time = 30  # â¸ï¸ Pausa de 30 segundos entre lotes\n",
    "total_rows = len(df)\n",
    "\n",
    "print(f\"ðŸš€ Total de canciones en el dataset: {total_rows}\")\n",
    "print(f\"ðŸ“Œ Procesando en bloques de {batch_size} canciones por batch con pausas de {pause_time}s.\")\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# ðŸ“Œ Generar embeddings en bloques con pausas\n",
    "for start in range(0, total_rows, batch_size):\n",
    "    end = min(start + batch_size, total_rows)\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nâš¡ [Batch {start}-{end}] Iniciando procesamiento...\")\n",
    "\n",
    "    text_batch = df.iloc[start:end]['combined_text'].tolist()\n",
    "\n",
    "    if len(text_batch) == 0:\n",
    "        print(f\"âŒ [Batch {start}-{end}] No hay datos para procesar. Saltando...\")\n",
    "        continue  \n",
    "\n",
    "    print(f\"â³ [Batch {start}-{end}] Generando embeddings en GPU...\")\n",
    "\n",
    "    try:\n",
    "        embeddings_batch = model.encode(\n",
    "            text_batch,\n",
    "            batch_size=16,  # ðŸ”¹ Se mantiene en 16 para no saturar GPU\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        embeddings_list.extend(embeddings_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en batch {start}-{end}: {str(e)}\")\n",
    "        continue  \n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"âœ… [Batch {start}-{end}] Embeddings generados en {elapsed_time:.2f} segundos.\\n\")\n",
    "\n",
    "    # â¸ï¸ Pausa para evitar sobrecalentamiento\n",
    "    print(f\"â³ Esperando {pause_time}s para enfriar la CPU/GPU...\\n\")\n",
    "    time.sleep(pause_time)\n",
    "\n",
    "# ðŸ“Œ Convertir embeddings a NumPy y crear FAISS index\n",
    "print(\"ðŸš€ Guardando embeddings y optimizando FAISS...\")\n",
    "\n",
    "embeddings_np = np.array(embeddings_list, dtype=np.float32)\n",
    "d = embeddings_np.shape[1]\n",
    "\n",
    "# ðŸ“Œ Crear Ã­ndice FAISS optimizado para bÃºsquedas rÃ¡pidas\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# ðŸ“Œ Guardar el Ã­ndice FAISS optimizado\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "\n",
    "print(f\"âœ… FAISS `IndexFlatIP` guardado en {faiss_index_path}, listo para bÃºsquedas semÃ¡nticas precisas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# ðŸ“Œ Ruta del archivo de embeddings generados con RoBERTa\n",
    "embeddings_file_roberta = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\embeddings_roberta2.pkl\"\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset con los embeddings\n",
    "df = pd.read_pickle(embeddings_file_roberta)\n",
    "\n",
    "# ðŸ“Œ Cargar modelo RoBERTa\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âš¡ Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('all-roberta-large-v1', device=device)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Traduce la frase del usuario a inglÃ©s si no estÃ¡ en inglÃ©s.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"ðŸŒ Traducido '{text}' âž '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    Busca canciones en base a una frase de usuario.\n",
    "    \n",
    "    - user_query: Texto ingresado por el usuario (ejemplo: \"Me gusta conducir en invierno\").\n",
    "    - top_n: NÃºmero de canciones recomendadas.\n",
    "    \"\"\"\n",
    "    # ðŸ“Œ Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # ðŸ“Œ Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # ðŸ“Œ Asegurar que los embeddings en el DataFrame son listas\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: x if isinstance(x, list) else list(x))\n",
    "\n",
    "    # ðŸ“Œ Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([x], [query_embedding])[0][0])\n",
    "\n",
    "    # ðŸ“Œ Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'similarity']]\n",
    "\n",
    "# ðŸ” Prueba con una consulta\n",
    "user_input = \"Comprad el libro de mi hermano, cabrones. Cuando sea famoso y tenga una serie en Netflix dirÃ©is queÂ loÂ habÃ©isÂ leÃ­do\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados\n",
    "print(resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tarda to much :\n",
    "Vamos a probar con cosine_similarity() en un solo paso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# ðŸ“Œ Ruta del archivo de embeddings generados con RoBERTa\n",
    "embeddings_file_roberta = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\embeddings_roberta2.pkl\"\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset con los embeddings\n",
    "df = pd.read_pickle(embeddings_file_roberta)\n",
    "\n",
    "# ðŸ“Œ Convertir embeddings a NumPy arrays (para mejor rendimiento)\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "# ðŸ“Œ Cargar modelo RoBERTa en GPU si estÃ¡ disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"âš¡ Usando: {device.upper()}\")\n",
    "\n",
    "model = SentenceTransformer('all-roberta-large-v1', device=device)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Traduce la frase del usuario a inglÃ©s si no estÃ¡ en inglÃ©s.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"ðŸŒ Traducido '{text}' âž '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "def translate_to_spanish(text):\n",
    "    \"\"\"Traduce una letra de inglÃ©s a espaÃ±ol.\"\"\"\n",
    "    return GoogleTranslator(source='en', target='es').translate(text) if isinstance(text, str) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    Busca canciones en base a una frase del usuario.\n",
    "    \n",
    "    - user_query: Texto ingresado por el usuario (ejemplo: \"Me gusta conducir en invierno\").\n",
    "    - top_n: NÃºmero de canciones recomendadas.\n",
    "    \"\"\"\n",
    "    # ðŸ“Œ Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # ðŸ“Œ Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # ðŸ“Œ Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([x], [query_embedding])[0][0])\n",
    "\n",
    "    # ðŸ“Œ Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n).copy()\n",
    "\n",
    "    # ðŸ“Œ Traducir las letras de las canciones al espaÃ±ol\n",
    "    top_songs['translated_lyrics'] = top_songs['processed_lyrics'].apply(lambda x: translate_to_spanish(x[:500]) if isinstance(x, str) else \"TraducciÃ³n no disponible\")\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'translated_lyrics', 'similarity']]\n",
    "\n",
    "# ðŸ” Prueba con una consulta\n",
    "user_input = \"he pedido matrimonio a mi novia durante el rodaje de una pelÃ­cula\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # ðŸ”¹ Asegurar que las letras sean cadenas de texto vÃ¡lidas\n",
    "    original_lyrics = str(row['processed_lyrics']) if isinstance(row['processed_lyrics'], str) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if isinstance(row['translated_lyrics'], str) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra en InglÃ©s:**\\n\", original_lyrics, \"...\")\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics)\n",
    "    print(\"=\"*500, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” PRUEBA de velocidad con una consulta\n",
    "user_input = \"Me encanta viajar sola\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original (Primeros 800 caracteres):**\\n\", lyrics[:800], \"...\")\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola (Primeros 800 caracteres):**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"sin mÃºsica la vida serÃ­a un error\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ” Prueba con una consulta en espaÃ±ol\n",
    "user_input = \"He cometido el peor de los pecados que un hombre puede cometer: no he sido feliz\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Prueba con una consulta\n",
    "user_input = \"hard metal feliz\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # ðŸ”¹ Asegurar que las letras sean cadenas de texto vÃ¡lidas\n",
    "    original_lyrics = str(row['processed_lyrics']) if isinstance(row['processed_lyrics'], str) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if isinstance(row['translated_lyrics'], str) else \"TraducciÃ³n no disponible\"\n",
    "\n",
    "    print(\"\\nðŸ“œ **Letra Original:**\\n\", original_lyrics[:500], \"...\")\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola:**\\n\", translated_lyrics)\n",
    "    print(\"=\"*80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO BertLyrics - Este modelo intentamos usarlo pero estaba disponible y se vio que Roberta estaba reentrenado con Ã©l acercÃ¡ndose mÃ¡s a nuestra finalidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "model_name = \"brunokreiner/lyrics-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Crear embedding para una frase de prueba\n",
    "sentence = \"I feel so lonely tonight\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener los embeddings de la Ãºltima capa\n",
    "embeddings = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Frase del usuario\n",
    "user_sentence = \"I feel so lonely tonight\"\n",
    "\n",
    "# TokenizaciÃ³n\n",
    "inputs = tokenizer(user_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Obtener embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraer el embedding del CLS token\n",
    "user_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "print(\"ðŸ“Œ Embedding generado:\", user_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ðŸ“Œ Ruta del dataset\n",
    "file_path = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\final_df.csv\"\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ðŸ“Œ Seleccionar solo la columna de letras procesadas\n",
    "df = df[['recording_id', 'processed_lyrics']].dropna()\n",
    "\n",
    "# ðŸ“Œ Cargar el modelo de Hugging Face\n",
    "model_name = \"brunokreiner/lyrics-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# ðŸ“Œ Pasar el modelo a GPU si estÃ¡ disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para generar embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# ðŸ“Œ Generar los embeddings para todas las canciones\n",
    "embeddings = []\n",
    "for lyrics in df['processed_lyrics']:\n",
    "    embedding = get_embedding(lyrics)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# ðŸ“Œ Convertir a numpy array y guardar\n",
    "embeddings_array = np.array(embeddings)\n",
    "np.save(\"lyrics_embeddings.npy\", embeddings_array)\n",
    "\n",
    "# ðŸ“Œ Guardar un dataframe con IDs y embeddings\n",
    "df_embeddings = pd.DataFrame({\n",
    "    'recording_id': df['recording_id'],\n",
    "    'embedding': list(embeddings)\n",
    "})\n",
    "df_embeddings.to_pickle(\"lyrics_embeddings.pkl\")\n",
    "\n",
    "print(\"âœ… Â¡Embeddings generados y guardados correctamente!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ðŸ“Œ Rutas de los archivos\n",
    "embeddings_file = \"lyrics_embeddings.pkl\"  # Archivo con embeddings\n",
    "metadata_file = \"final_df.csv\"  # Archivo con datos completos de las canciones\n",
    "model_name = \"brunokreiner/lyrics-bert\"  # Modelo\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset con embeddings\n",
    "df_embeddings = pd.read_pickle(embeddings_file)\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset completo con metadatos (nombre de canciÃ³n, artista, etc.)\n",
    "df_metadata = pd.read_csv(metadata_file)\n",
    "\n",
    "# ðŸ“Œ Fusionar embeddings con metadatos usando \"recording_id\"\n",
    "df = df_embeddings.merge(df_metadata, on=\"recording_id\", how=\"left\")\n",
    "\n",
    "# ðŸ“Œ Cargar modelo y tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para traducir la frase del usuario a inglÃ©s si es necesario\n",
    "def translate_to_english(text):\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':  \n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"ðŸŒ Traducido '{text}' âž '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para obtener el embedding de la frase del usuario\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para traducir los primeros caracteres de la letra de la canciÃ³n al espaÃ±ol\n",
    "def translate_lyrics_snippet(lyrics):\n",
    "    if pd.notna(lyrics) and isinstance(lyrics, str):\n",
    "        snippet = lyrics[:300]  # Primeros 300 caracteres\n",
    "        translated_snippet = GoogleTranslator(source='en', target='es').translate(snippet)\n",
    "        return translated_snippet\n",
    "    return \"TraducciÃ³n no disponible\"\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para buscar canciones segÃºn la frase del usuario\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"Encuentra canciones en base a la frase ingresada por el usuario\"\"\"\n",
    "    \n",
    "    # ðŸ”Ž Traducir la frase si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "    \n",
    "    # ðŸ”Ž Convertir la frase en embedding\n",
    "    query_embedding = get_embedding(translated_query).reshape(1, -1)\n",
    "    \n",
    "    # ðŸ“Œ Calcular similitudes con todas las canciones\n",
    "    song_embeddings = np.vstack(df['embedding'].values)  # Convertir embeddings a NumPy array\n",
    "    similarities = cosine_similarity(query_embedding, song_embeddings)[0]\n",
    "    \n",
    "    # ðŸ“Œ Agregar la similitud al dataframe\n",
    "    df['similarity'] = similarities\n",
    "    \n",
    "    # ðŸ“Œ Ordenar por mayor similitud y seleccionar las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "    \n",
    "    return top_songs[['recording_id', 'song_name', 'artist_name', 'spotify_url', 'processed_lyrics', 'similarity']]\n",
    "\n",
    "# ðŸ” Prueba con una frase del usuario\n",
    "user_input = \"mis amigos son los mejores\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados en tu formato preferido\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify URL:** {row['spotify_url'] if pd.notna(row['spotify_url']) else 'Sin URL'}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Mostrar letra original (mÃ¡ximo 800 caracteres)\n",
    "    lyrics = row['processed_lyrics'] if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    print(\"\\nðŸ“œ **Letra Original (Primeros 800 caracteres):**\")\n",
    "    print(lyrics[:800] + \"...\" if len(lyrics) > 800 else lyrics)\n",
    "\n",
    "    # Traducir los primeros 300 caracteres de la letra\n",
    "    translated_snippet = translate_lyrics_snippet(lyrics)\n",
    "    print(\"\\nðŸŒ **TraducciÃ³n EspaÃ±ola (Primeros 300 caracteres):**\")\n",
    "    print(translated_snippet)\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ðŸ“Œ Rutas de los archivos\n",
    "embeddings_file = \"lyrics_embeddings.pkl\"  # Archivo con los embeddings de las canciones\n",
    "model_name = \"brunokreiner/lyrics-bert\"  # Modelo\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset con los embeddings\n",
    "df = pd.read_pickle(embeddings_file)\n",
    "\n",
    "# ðŸ“Œ Cargar modelo y tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para obtener el embedding de la frase del usuario\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para buscar canciones segÃºn la frase del usuario\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"Encuentra canciones en base a la frase ingresada por el usuario\"\"\"\n",
    "    \n",
    "    # ðŸ”Ž Convertir la frase en embedding\n",
    "    query_embedding = get_embedding(user_query).reshape(1, -1)\n",
    "    \n",
    "    # ðŸ“Œ Calcular similitudes con todas las canciones\n",
    "    song_embeddings = np.vstack(df['embedding'].values)  # Convertir embeddings a NumPy array\n",
    "    similarities = cosine_similarity(query_embedding, song_embeddings)[0]\n",
    "    \n",
    "    # ðŸ“Œ Agregar la similitud al dataframe\n",
    "    df['similarity'] = similarities\n",
    "    \n",
    "    # ðŸ“Œ Ordenar por mayor similitud y seleccionar las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "    \n",
    "    return top_songs[['recording_id', 'song_name', 'artist_name', 'spotify_url', 'processed_lyrics', 'similarity']]\n",
    "\n",
    "# ðŸ” Prueba con una frase del usuario\n",
    "user_input = \"I feel broken and alone\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados en tu formato preferido\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify URL:** {row['spotify_url'] if pd.notna(row['spotify_url']) else 'Sin URL'}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Mostrar letra (mÃ¡ximo 800 caracteres)\n",
    "    lyrics = row['processed_lyrics'] if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    print(\"\\nðŸ“œ **Letra Original (Primeros 800 caracteres):**\")\n",
    "    print(lyrics[:800] + \"...\" if len(lyrics) > 800 else lyrics)\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decarto este"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RED NEURONAL PARA JUNTAR EMBEDDINGS DE T5 - ROBERTA Y MPNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ðŸ“Œ Cargar datasets con embeddings de diferentes modelos\n",
    "df_t5 = pd.read_pickle(\"clean_df_embeddings_t5.pkl\")\n",
    "df_roberta = pd.read_pickle(\"clean_df_embeddings_roberta.pkl\")\n",
    "df_mpnet = pd.read_pickle(\"clean_df_embeddings_768.pkl\")  # MPNet\n",
    "\n",
    "# ðŸ“Œ Unir los datasets usando recording_id como clave\n",
    "df = df_t5.merge(df_roberta, on=\"recording_id\", suffixes=(\"_t5\", \"_roberta\"))\n",
    "df = df.merge(df_mpnet, on=\"recording_id\")\n",
    "\n",
    "# ðŸ“Œ Convertir embeddings en arrays NumPy y normalizar\n",
    "def normalize_embedding(embedding_list):\n",
    "    embedding_array = np.array(embedding_list)\n",
    "    return embedding_array / np.linalg.norm(embedding_array)\n",
    "\n",
    "df[\"embedding_t5\"] = df[\"embedding_t5\"].apply(normalize_embedding)\n",
    "df[\"embedding_roberta\"] = df[\"embedding_roberta\"].apply(normalize_embedding)\n",
    "df[\"embedding_mpnet\"] = df[\"embedding\"].apply(normalize_embedding)  # MPNet\n",
    "\n",
    "# ðŸ“Œ Concatenar los embeddings en un solo vector\n",
    "df[\"combined_embedding\"] = df.apply(lambda row: np.concatenate([\n",
    "    row[\"embedding_t5\"],\n",
    "    row[\"embedding_roberta\"],\n",
    "    row[\"embedding_mpnet\"]\n",
    "]), axis=1)\n",
    "\n",
    "# ðŸ“Œ Guardar el dataset combinado\n",
    "df[[\"recording_id\", \"combined_embedding\"]].to_pickle(\"clean_df_embeddings_combined.pkl\")\n",
    "\n",
    "print(\"âœ… Embeddings fusionados y guardados correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ðŸ“Œ Rutas de tus archivos\n",
    "csv_file = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df.csv\"\n",
    "embeddings_file = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_combined.pkl\"\n",
    "\n",
    "# ðŸ“Œ Cargar dataset\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# ðŸ“Œ Cargar embeddings\n",
    "df_embeddings = pd.read_pickle(embeddings_file)\n",
    "\n",
    "# ðŸ“Œ Unir ambos datasets por 'recording_id'\n",
    "df = df.merge(df_embeddings, on=\"recording_id\", how=\"inner\")\n",
    "\n",
    "# ðŸ“Œ Convertir embeddings a un array NumPy (FAISS necesita float32)\n",
    "embedding_matrix = np.vstack(df['combined_embedding'].values).astype(\"float32\")\n",
    "\n",
    "# ðŸ“Œ Crear Ã­ndice FAISS\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])  # Ãndice de similitud L2\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# ðŸ“Œ Cargar modelo para obtener embeddings de consulta\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "def get_combined_embedding(text):\n",
    "    \"\"\"Genera el embedding combinado usando los mismos modelos que se usaron en FAISS\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mpnet_output = mpnet_model(**inputs).last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "        t5_output = t5_model(**inputs).last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "        roberta_output = roberta_model(**inputs).last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "    # ðŸ“Œ Concatenar los embeddings en un solo vector de 2560 dimensiones (768+768+1024)\n",
    "    combined_embedding = np.concatenate([mpnet_output, t5_output, roberta_output])\n",
    "\n",
    "    return combined_embedding\n",
    "\n",
    "\n",
    "# ðŸ“Œ FunciÃ³n para obtener embedding de la frase del usuario\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().astype(\"float32\")\n",
    "\n",
    "def search_songs_faiss(user_query, top_n=5):\n",
    "    query_embedding = get_combined_embedding(user_query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, top_n)  # FAISS realiza la bÃºsqueda\n",
    "\n",
    "    # ðŸ“Œ Extraer resultados del dataframe\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    return results\n",
    "\n",
    "\n",
    "# ðŸ” Prueba con una frase del usuario\n",
    "user_input = \"I feel broken and alone\"\n",
    "resultados = search_songs_faiss(user_input, top_n=5)\n",
    "\n",
    "# ðŸ“Œ Mostrar resultados en tu formato preferido\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸŽµ **CanciÃ³n:** {row['song_name']}\")\n",
    "    print(f\"ðŸŽ¤ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"ðŸ”— **Spotify URL:** {row['spotify_url'] if pd.notna(row['spotify_url']) else 'Sin URL'}\")\n",
    "    print(f\"âœ… **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Mostrar letra (mÃ¡ximo 800 caracteres)\n",
    "    lyrics = row['processed_lyrics'] if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    print(\"\\nðŸ“œ **Letra Original (Primeros 800 caracteres):**\")\n",
    "    print(lyrics[:800] + \"...\" if len(lyrics) > 800 else lyrics)\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Verificar dimensiÃ³n del Ã­ndice FAISS\n",
    "print(f\"FAISS Index Dim: {index.d}\")\n",
    "\n",
    "# ðŸ“Œ Generar el embedding de la consulta y verificar su dimensiÃ³n\n",
    "query_embedding = get_embedding(\"I feel broken and alone\").reshape(1, -1)\n",
    "print(f\"Query Embedding Dim: {query_embedding.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_embeddings.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarda demasiado - pruebo faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DescripciÃ³n estadÃ­stica de las columnas numÃ©ricas\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
