{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducci√≥n\n",
    "En este notebook, exploramos diferentes modelos de procesamiento de lenguaje natural (NLP) para construir un recomendador de canciones basado en letras y estados de √°nimo. Evaluamos varios modelos y finalmente elegimos el que mejor se adapta a nuestras necesidades.\n",
    "\n",
    "**Quisiera pedir perd√≥n ya que es un notebook recuperado que ha perdido su mayor esencia y mucho del trabajo que llevaba (gajes de ser nueva en el mundo git)**\n",
    "\n",
    "En el cuaderno [modelos_perdido.ipynb](functions/4%20-%20Recomendador%20letras%20canciones/modelos_perdido.ipynb) corrompido, salen algunos de los resultados de los modelos, aunque las m√©tricas de algunos eran mayores en similaridad el que m√°s nos ha convencido finalmente ha sido el de Roberta el cual en cuaderno [Berta_Final.ipynb](functions/4%20-%20Recomendador%20letras%20canciones/Berta_Final.ipynb) podr√©is ver que lo hemos mezclado con etiqutas FAISS para una b√∫squeda m√°s r√°pida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Comparaci√≥n de Modelos para B√∫squeda Sem√°ntica de Canciones\n",
    "\n",
    "| **Modelo**                  | **Ventajas**                                         | **Desventajas**                                       | **Recomendado?** | **Dimensi√≥n Embeddings** |\n",
    "|-----------------------------|----------------------------------------------------|-----------------------------------------------------|-------------------------|--------------------------|\n",
    "| **‚úÖ all-roberta-large-v1**  | üîπ Muy preciso en textos largos. üîπ Captura contexto profundo. | üîπ M√°s lento en inferencia. üîπ Requiere m√°s RAM/GPU. | ‚úÖ **S√≠, es el modelo que hemos elegido por su alta precisi√≥n.** | 1024 |\n",
    "| **‚úÖ sentence-t5-base**      | üîπ Optimizado para b√∫squeda sem√°ntica. üîπ Puede generar textos (opcional). | üîπ No est√° entrenado en datos musicales. | ‚úÖ **S√≠, es nuestra segunda opci√≥n por su equilibrio entre precisi√≥n y eficiencia.** | 768 |\n",
    "| **üî• all-mpnet-base-v2**     | üîπ Muy equilibrado: r√°pido y preciso. üîπ Optimizado para comparaci√≥n sem√°ntica. | üîπ No es el m√°s avanzado, pero tiene buen rendimiento. | ‚ö†Ô∏è **Tal vez, si queremos una opci√≥n r√°pida y precisa.** | 768 |\n",
    "| **üî• all-mpnet-large**       | üîπ Mismo modelo que usamos pero m√°s preciso. üîπ Optimizado para comprensi√≥n profunda. | üîπ Puede ser m√°s lento que `mpnet-base-v2`. | ‚ö†Ô∏è **Roberta Nos gusta m√°s para nuestra finalidad.** | 1024 |\n",
    "| **üî• all-distilroberta-v1**  | üîπ M√°s r√°pido que `roberta-large`. üîπ Muy bueno en b√∫squeda sem√°ntica. | üîπ No es tan profundo como `roberta-large`. | ‚ö†Ô∏è **Tal vez, si necesitamos m√°s velocidad sin perder mucha precisi√≥n.** | 768 |\n",
    "| **Bert**                    | üîπ Bien entrenado en datos generales. üîπ Buen rendimiento en tareas generales de NLP. | üîπ No tan optimizado para datos musicales espec√≠ficos. | ‚ùå **No, preferimos modelos m√°s especializados.** | 768 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intento de Red Neuronal para Juntar Embeddings de T5, RoBERTa y MPNet\n",
    "Se intent√≥ utilizar una red neuronal para combinar los embeddings generados por los modelos T5, RoBERTa y MPNet con el objetivo de mejorar la precisi√≥n de las recomendaciones. Sin embargo, este enfoque no result√≥ ser eficiente en t√©rminos de tiempo de procesamiento y rendimiento. Por lo tanto, se decidi√≥ no utilizar esta t√©cnica en la implementaci√≥n final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarte del Clustering y Formateo Total del Texto\n",
    "\n",
    "### Clustering\n",
    "Inicialmente, consideramos utilizar t√©cnicas de clustering para agrupar canciones con letras similares. Sin embargo, este enfoque fue descartado por varias razones:\n",
    "\n",
    "1. **Complejidad Computacional:** El clustering de grandes vol√∫menes de datos textuales requiere una cantidad significativa de recursos computacionales, lo que puede ser ineficiente y lento.\n",
    "2. **P√©rdida de Informaci√≥n Sem√°ntica:** Los m√©todos de clustering tradicionales pueden no capturar adecuadamente la riqueza sem√°ntica y el contexto de las letras de las canciones, lo que resulta en agrupaciones menos precisas.\n",
    "3. **Dificultad en la Interpretaci√≥n:** Los resultados del clustering pueden ser dif√≠ciles de interpretar y explicar, especialmente cuando se trata de datos textuales complejos como las letras de canciones.\n",
    "\n",
    "### Formateo Total del Texto\n",
    "Tambi√©n se prob√≥ a realizar un formateo exhaustivo de las letras de las canciones para normalizar el texto. Sin embargo, este enfoque fue descartado por las siguientes razones:\n",
    "\n",
    "1. **P√©rdida de Contexto:** El formateo excesivo puede eliminar informaci√≥n contextual importante, como la estructura po√©tica y las expresiones idiom√°ticas, que son cruciales para la comprensi√≥n sem√°ntica.\n",
    "2. **Complejidad del Preprocesamiento:** Implementar un formateo total del texto requiere un preprocesamiento complejo y detallado, lo que puede ser propenso a errores y dif√≠cil de mantener.\n",
    "3. **Impacto Negativo en el Rendimiento del Modelo:** La normalizaci√≥n excesiva del texto puede afectar negativamente el rendimiento de los modelos de lenguaje, que est√°n dise√±ados para manejar variaciones en el texto natural.\n",
    "\n",
    "En lugar de estos enfoques, optamos por utilizar modelos de lenguaje avanzados como **all-roberta-large-v1** y **sentence-t5-base**, que pueden capturar de manera m√°s efectiva la sem√°ntica y el contexto de las letras de las canciones sin necesidad de un preprocesamiento excesivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar Librer√≠as\n",
    "Importamos las librer√≠as necesarias para el an√°lisis y procesamiento de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Debe devolver True si hay GPU disponible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el Dataset\n",
    "Cargamos el archivo CSV que contiene las letras de las canciones y otros metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV\n",
    "file_path = r'C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\final_df.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informaci√≥n del Dataset\n",
    "Mostramos informaci√≥n general del dataset, incluyendo el n√∫mero de entradas y las columnas disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Datos\n",
    "Realizamos una limpieza b√°sica de los datos, eliminando duplicados y manejando valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Manejar valores nulos\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de Texto\n",
    "Definimos funciones para limpiar y formatear las letras de las canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Convertir a min√∫sculas\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplazar m√∫ltiples espacios con uno solo\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Eliminar caracteres especiales\n",
    "    text = text.strip()  # Eliminar espacios al inicio y final\n",
    "    return text\n",
    "\n",
    "def format_text_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_format = ['song_name', 'artist_name', 'album_name', 'playlists_names', 'combined_genres', 'processed_lyrics']\n",
    "df = format_text_columns(df, columns_to_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploraci√≥n de Datos\n",
    "Realizamos una exploraci√≥n inicial de los datos para entender mejor su estructura y contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          artist_name                song_name  \\\n",
    "91   birds of chicago         estrella goodbye   \n",
    "109         black rob              muscle game   \n",
    "134      balsam range          trains i missed   \n",
    "135            lights     cactus in the valley   \n",
    "167    ray lamontagne         change your mind   \n",
    "181     steve winwood           theres a river   \n",
    "190      the vaccines           young american   \n",
    "280         alpha 520     les larmes du soleil   \n",
    "300             loote   high without your love   \n",
    "375        tom t hall  legend of the lady bear   \n",
    "\n",
    "                                               spotify_url  \\\n",
    "91   https://open.spotify.com/track/5jcetnvnAWGTMGufJ7oEnc   \n",
    "109  https://open.spotify.com/track/5xYb779c1wmOmbfaexX8JK   \n",
    "134  https://open.spotify.com/track/1ap81IwnNa5QeuvMMnbZfu   \n",
    "135  https://open.spotify.com/track/288xqEJiCcSc2zGFanlclV   \n",
    "167  https://open.spotify.com/track/66r7ecZd8LGtJWb9t6PhA6   \n",
    "181  https://open.spotify.com/track/6IbNMlVEr2GY2DpkxINOts   \n",
    "190  https://open.spotify.com/track/4gcZVTYjOBMh1UVlZy2YJW   \n",
    "280  https://open.spotify.com/track/3ItQEhTWSvMOHTinqdxGB0   \n",
    "300  https://open.spotify.com/track/3hwyRkhcVkcgUJHKKmxS8k   \n",
    "375  https://open.spotify.com/track/4jWcTUjobvn6pzgN3b7w7l  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informaci√≥n del Dataset\n",
    "Mostramos informaci√≥n general del dataset, incluyendo el n√∫mero de entradas y las columnas disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()  # Convertir a min√∫sculas\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplazar m√∫ltiples espacios con uno solo\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Eliminar caracteres especiales\n",
    "    text = text.strip()  # Eliminar espacios al inicio y final\n",
    "    return text\n",
    "\n",
    "def format_text_columns(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(str).apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_format = ['song_name', 'artist_name', 'album_name', 'playlists_names', 'combined_genres', 'processed_lyrics']\n",
    "df = format_text_columns(df, columns_to_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nulos\n",
    "pd.set_option('display.max_rows', None) \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 30) \n",
    "# visualizar contenido de la celda spotify_url entero cuando se filtra en pandas\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar canciones con \"mood_sad\" alto y palabras clave en las letras\n",
    "keywords = [\"playa\", \"mar\", \"oc√©ano\", \"olas\", \"arena\", \"brisa\", \"agua\"]\n",
    "filtered_songs = df[(df['mood_sad'] > 0.7) & df['processed_lyrics'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "\n",
    "# Mostrar las primeras 10 filas\n",
    "print(filtered_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRUEBAS DE MODELOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# all-MiniLM-L6-v2 - 384 Emb\n",
    "\n",
    "**Descripci√≥n del Modelo:**\n",
    "El modelo **all-MiniLM-L6-v2** es una versi√≥n ligera y eficiente de los modelos de lenguaje. Est√° dise√±ado para generar embeddings de alta calidad con una dimensi√≥n de 384. Este modelo es conocido por su rapidez y eficiencia en el uso de recursos, lo que lo hace ideal para aplicaciones en tiempo real y sistemas con limitaciones de memoria y procesamiento.\n",
    "\n",
    "**Ventajas:**\n",
    "- **R√°pido y Ligero:** Debido a su menor tama√±o, el modelo es extremadamente r√°pido en la generaci√≥n de embeddings y consume menos memoria RAM y GPU.\n",
    "- **Eficiente:** Ideal para aplicaciones que requieren respuestas r√°pidas y en tiempo real, como chatbots y sistemas de b√∫squeda r√°pida.\n",
    "\n",
    "**Desventajas:**\n",
    "- **Menor Precisi√≥n:** Aunque es r√°pido y eficiente, la precisi√≥n de los embeddings generados por **all-MiniLM-L6-v2** es menor en comparaci√≥n con modelos m√°s grandes y complejos.\n",
    "- **Menor Capacidad de Comprensi√≥n:** No captura el contexto y la sem√°ntica de los textos tan bien como otros modelos m√°s avanzados.\n",
    "\n",
    "**Raz√≥n para No Usarlo en el Proyecto:**\n",
    "Para este proyecto, que implica la recomendaci√≥n de canciones basadas en letras y estados de √°nimo o frases, se requiere un modelo que pueda capturar de manera precisa y profunda el contexto y la sem√°ntica de las letras de las canciones. Aunque **all-MiniLM-L6-v2** es r√°pido y eficiente, su menor precisi√≥n y capacidad de comprensi√≥n no son suficientes para las necesidades del proyecto. Modelos como **all-roberta-large-v1** y **sentence-t5-base** ofrecen una mejor comprensi√≥n sem√°ntica y precisi√≥n, lo que los hace m√°s adecuados para este tipo de tareas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# üìå Cargar modelo y GPU \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# üìå Cargar dataset con embeddings\n",
    "embeddings_file = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings.pkl\"\n",
    "df = pd.read_pickle(embeddings_file)\n",
    "\n",
    "# üìå Convertir embeddings a arrays NumPy para mejorar rendimiento\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Detecta el idioma y traduce al ingl√©s si es necesario.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':  # Si el texto no est√° en ingl√©s, traducimos\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"üåç Traducido '{text}' ‚ûù '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text  # Si ya est√° en ingl√©s, no hacemos nada\n",
    "\n",
    "def translate_to_spanish(text):\n",
    "    \"\"\"Traduce la letra de la canci√≥n al espa√±ol.\"\"\"\n",
    "    if pd.notna(text):  # Solo traducimos si hay letra disponible\n",
    "        return GoogleTranslator(source='en', target='es').translate(text)\n",
    "    return \"Traducci√≥n no disponible\"\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    B√∫squeda sem√°ntica de canciones con traducci√≥n autom√°tica si el usuario escribe en otro idioma.\n",
    "    \"\"\"\n",
    "    # üìå Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # üìå Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "    # üìå Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity(x.reshape(1, -1), query_embedding)[0][0])\n",
    "\n",
    "    # üìå Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n).copy()\n",
    "\n",
    "    # üìå Traducir las letras de las canciones al espa√±ol\n",
    "    top_songs['translated_lyrics'] = top_songs['processed_lyrics'].apply(lambda x: translate_to_spanish(x[:500]) if isinstance(x, str) else \"\")\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'translated_lyrics', 'similarity']]\n",
    "\n",
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"Canciones para bailar con mis amigas\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*80, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"me encanta viajar sola\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*80, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"sin m√∫sica la vida ser√≠a un error\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*80, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"He cometido el peor de los pecados que un hombre puede cometer: no he sido feliz\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all-mpnet-base-v2 - 768 Emb\n",
    "\n",
    "**Descripci√≥n del Modelo:**\n",
    "El modelo **all-mpnet-base-v2** es conocido por su equilibrio entre precisi√≥n y eficiencia. Genera embeddings de alta calidad con una dimensi√≥n de 768, lo que lo hace adecuado para una variedad de tareas de procesamiento de lenguaje natural, incluyendo la b√∫squeda sem√°ntica y la comparaci√≥n de textos.\n",
    "\n",
    "**Ventajas:**\n",
    "- **Alta Precisi√≥n:** Ofrece una excelente precisi√≥n en la comprensi√≥n sem√°ntica de textos.\n",
    "- **Eficiencia:** Es m√°s r√°pido y consume menos recursos que modelos m√°s grandes como **all-roberta-large-v1**.\n",
    "- **Versatilidad:** Adecuado para tareas de b√∫squeda sem√°ntica y comparaci√≥n de textos.\n",
    "\n",
    "**Desventajas:**\n",
    "- **Menor Capacidad de Comprensi√≥n:** Aunque es preciso, no captura el contexto y la sem√°ntica tan profundamente como modelos m√°s grandes.\n",
    "- **Limitaciones en Textos Largos:** Puede no ser tan efectivo en la comprensi√≥n de textos muy largos y detallados.\n",
    "\n",
    "**Raz√≥n para No Usarlo en el Proyecto:**\n",
    "Para este proyecto, que implica la recomendaci√≥n de canciones basadas en letras y estados de √°nimo, se requiere un modelo que pueda capturar de manera precisa y profunda el contexto y la sem√°ntica de las letras de las canciones. Aunque **all-mpnet-base-v2** funciona dando unas similitudes m√°s alta, su capacidad de comprensi√≥n sem√°ntica es menor en comparaci√≥n con modelos m√°s avanzados como **all-roberta-large-v1** y **sentence-t5-base**. Estos modelos ofrecen una mejor comprensi√≥n sem√°ntica y precisi√≥n, lo que los hace m√°s adecuados para las necesidades del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# üìå Ruta del dataset con los embeddings generados\n",
    "embeddings_file_768 = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_768.pkl\"\n",
    "\n",
    "# üìå Cargar el dataset con embeddings\n",
    "df = pd.read_pickle(embeddings_file_768)\n",
    "\n",
    "# üìå Convertir embeddings a NumPy arrays para mejorar rendimiento\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "# üìå Cargar el modelo (mismo que usamos para generar embeddings)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "\n",
    "# üìå Funci√≥n para traducir la frase del usuario si no est√° en ingl√©s\n",
    "def translate_to_english(text):\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':  \n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"üåç Traducido '{text}' ‚ûù '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text  \n",
    "\n",
    "# üìå Funci√≥n para traducir las letras de las canciones al espa√±ol\n",
    "def translate_to_spanish(text):\n",
    "    if pd.notna(text):  # Solo traducimos si hay letra disponible\n",
    "        return GoogleTranslator(source='en', target='es').translate(text)\n",
    "    return \"Traducci√≥n no disponible\"\n",
    "\n",
    "# üìå Funci√≥n para buscar canciones seg√∫n una frase de usuario\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    Encuentra las mejores coincidencias en el dataset seg√∫n la frase ingresada por el usuario.\n",
    "    - user_query: Texto del usuario.\n",
    "    - top_n: N√∫mero de canciones recomendadas.\n",
    "    \"\"\"\n",
    "    # üìå Traducir la frase si no est√° en ingl√©s\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # üìå Convertir la frase en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # üìå Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([x], [query_embedding])[0][0])\n",
    "\n",
    "    # üìå Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n).copy()\n",
    "\n",
    "    # üìå Traducir las letras de las canciones al espa√±ol\n",
    "    top_songs['translated_lyrics'] = top_songs['processed_lyrics'].apply(\n",
    "        lambda x: translate_to_spanish(x[:500]) if isinstance(x, str) else \"Traducci√≥n no disponible\"\n",
    "    )\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'translated_lyrics', 'similarity']]\n",
    "\n",
    "# üîç Prueba con una frase del usuario\n",
    "user_input = \"canciones para bailar con mis amigas\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:500], \"...\")\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics)\n",
    "    print(\"=\"*100, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una frase del usuario\n",
    "user_input = \"Me encanta viajar sola\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:500], \"...\")\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics)\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"sin m√∫sica la vida ser√≠a un error\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"He cometido el peor de los pecados que un hombre puede cometer: no he sido feliz\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-t5-base\n",
    "\n",
    "**Descripci√≥n del Modelo:**\n",
    "El modelo **sentence-t5-base** es conocido por su capacidad para comprender y generar texto de manera eficiente. Est√° optimizado para tareas de b√∫squeda sem√°ntica y generaci√≥n de texto, lo que lo hace muy vers√°til en aplicaciones de procesamiento de lenguaje natural.\n",
    "\n",
    "**Ventajas:**\n",
    "- **Comprensi√≥n Sem√°ntica:** Excelente en la comprensi√≥n del significado completo de las frases, no solo de palabras individuales.\n",
    "- **Generaci√≥n de Texto:** Puede generar texto coherente y relevante, lo que es √∫til para tareas que requieren respuestas generativas.\n",
    "- **Equilibrio:** Ofrece un buen balance entre precisi√≥n y eficiencia, siendo m√°s r√°pido que modelos m√°s grandes.\n",
    "\n",
    "**Desventajas:**\n",
    "- **No Espec√≠fico para M√∫sica:** Aunque es potente en comprensi√≥n sem√°ntica, no est√° espec√≠ficamente entrenado en datos musicales, lo que puede limitar su rendimiento en tareas muy especializadas.\n",
    "- **Menor Precisi√≥n en Conceptos Abstractos:** Aunque es bueno en comprensi√≥n sem√°ntica, puede no captar tan bien los conceptos abstractos y matices como otros modelos m√°s avanzados.\n",
    "\n",
    "**Raz√≥n para No Usarlo:**\n",
    "Aunque **sentence-t5-base** fue nuestra segunda opci√≥n debido a su capacidad para comprender el significado completo de las frases, finalmente optamos por **all-roberta-large-v1**. Los resultados obtenidos con **all-roberta-large-v1** fueron significativamente mejores en la captura de conceptos abstractos y matices en las letras de las canciones. La capacidad de **all-roberta-large-v1** para entender el contexto profundo y los detalles de las letras nos convenci√≥ de que era la mejor opci√≥n para nuestro recomendador de canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# üìå Ruta del dataset y archivo de embeddings\n",
    "file_path = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df.csv\"\n",
    "embeddings_file_t5 = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_t5.pkl\"\n",
    "\n",
    "# üìå Cargar dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# üìå Crear la columna 'embedding' si no existe\n",
    "if 'embedding' not in df.columns:\n",
    "    df['embedding'] = None  # Evita errores al asignar embeddings m√°s tarde\n",
    "\n",
    "# üìå Usar GPU si est√° disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Usando: {device.upper()}\")  \n",
    "model = SentenceTransformer('sentence-t5-base', device=device)  # üìå Modelo con m√°xima calidad sem√°ntica\n",
    "\n",
    "# üìå Crear texto combinado para embeddings\n",
    "df['combined_text'] = (\n",
    "    df['song_name'].astype(str) + \" \" +\n",
    "    df['artist_name'].astype(str) + \" \" +\n",
    "    df['processed_lyrics'].astype(str) + \" \" +\n",
    "    df['combined_genres'].astype(str) + \" \" +\n",
    "    df['playlists_names'].astype(str) + \" \" +\n",
    "    df['album_release_date'].astype(str) + \" \" +\n",
    "    df['language'].astype(str)\n",
    ")\n",
    "\n",
    "# üìå Par√°metros de procesamiento en bloques\n",
    "batch_size = 10000  \n",
    "total_rows = len(df)\n",
    "\n",
    "print(f\"üöÄ Total de canciones en el dataset: {total_rows}\")\n",
    "print(f\"üìå Procesando en bloques de {batch_size} canciones por batch.\")\n",
    "\n",
    "# üìå Generar embeddings en bloques de 10,000 canciones\n",
    "for start in range(0, total_rows, batch_size):\n",
    "    end = min(start + batch_size, total_rows)\n",
    "    start_time = time.time()  # ‚è≥ Iniciar contador de tiempo\n",
    "    print(f\"\\n‚ö° [Batch {start}-{end}] Iniciando procesamiento...\")\n",
    "\n",
    "    # üìå Obtener lista de textos\n",
    "    text_batch = df.iloc[start:end]['combined_text'].tolist()\n",
    "\n",
    "    # üìå Evitar procesar si el lote est√° vac√≠o\n",
    "    if len(text_batch) == 0:\n",
    "        print(f\"‚ùå [Batch {start}-{end}] No hay datos para procesar. Saltando...\")\n",
    "        continue  \n",
    "\n",
    "    print(f\"‚è≥ [Batch {start}-{end}] Generando embeddings en GPU...\")\n",
    "\n",
    "    try:\n",
    "        embeddings_batch = model.encode(\n",
    "            text_batch,\n",
    "            batch_size=32,  # üî• M√°xima calidad sin sobrecargar GPU\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en batch {start}-{end}: {str(e)}\")\n",
    "        continue  \n",
    "\n",
    "    print(f\"‚úÖ [Batch {start}-{end}] Embeddings generados. Guardando...\")\n",
    "\n",
    "    # üìå Convertir cada embedding a lista\n",
    "    embeddings_batch_list = [emb.tolist() for emb in embeddings_batch]  \n",
    "    num_rows = end - start  # N√∫mero de filas en este batch\n",
    "\n",
    "    # üìå Ajustar el tama√±o de la lista si hay desajuste con el n√∫mero de filas\n",
    "    if len(embeddings_batch_list) > num_rows:\n",
    "        embeddings_batch_list = embeddings_batch_list[:num_rows]  \n",
    "    elif len(embeddings_batch_list) < num_rows:\n",
    "        embeddings_batch_list += [[None] * len(embeddings_batch_list[0])] * (num_rows - len(embeddings_batch_list))\n",
    "\n",
    "    # üìå Asignar los embeddings al DataFrame usando iloc y pd.Series\n",
    "    df.iloc[start:end, df.columns.get_loc('embedding')] = pd.Series(embeddings_batch_list, index=df.index[start:end])\n",
    "\n",
    "    # üìå Guardar en archivo para evitar p√©rdida de progreso\n",
    "    df.to_pickle(embeddings_file_t5)\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # ‚è± Tiempo tomado\n",
    "    print(f\"‚úÖ [Batch {start}-{end}] Guardado parcial exitoso en {elapsed_time:.2f} segundos.\\n\")\n",
    "\n",
    "    # üìå Peque√±a pausa para evitar sobrecarga de GPU\n",
    "    time.sleep(2)  \n",
    "\n",
    "print(\"üöÄ Todos los embeddings generados con el nuevo modelo y guardados correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# üìå Ruta del archivo de embeddings\n",
    "embeddings_file_t5 = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_t5.pkl\"\n",
    "\n",
    "# üìå Cargar el dataset con los embeddings\n",
    "df = pd.read_pickle(embeddings_file_t5)\n",
    "\n",
    "# üìå Cargar modelo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('sentence-t5-base', device=device)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Traduce la frase del usuario a ingl√©s si no est√° en ingl√©s.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"üåç Traducido '{text}' ‚ûù '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    Busca canciones en base a una frase de usuario.\n",
    "    \n",
    "    - user_query: Texto ingresado por el usuario (ejemplo: \"Me gusta conducir en invierno\").\n",
    "    - top_n: N√∫mero de canciones recomendadas.\n",
    "    \"\"\"\n",
    "    # üìå Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # üìå Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # üìå Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([x], [query_embedding])[0][0])\n",
    "\n",
    "    # üìå Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'similarity']]\n",
    "\n",
    "# üîç Prueba con una consulta\n",
    "user_input = \"mis amigos y amigas son los mejores\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados\n",
    "print(resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# üìå Cargar el dataset con los embeddings\n",
    "embeddings_file_t5 = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_t5.pkl\"\n",
    "df = pd.read_pickle(embeddings_file_t5)\n",
    "\n",
    "# üìå Convertir embeddings a array NumPy (MUY IMPORTANTE PARA RENDIMIENTO)\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "embedding_matrix = np.vstack(df['embedding'].values)  # Convierte todos los embeddings en un array\n",
    "\n",
    "# üìå Cargar modelo en GPU si est√° disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('sentence-t5-base', device=device)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Traduce la frase del usuario a ingl√©s si no est√° en ingl√©s.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"üåç Traducido '{text}' ‚ûù '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "def translate_to_spanish(text):\n",
    "    \"\"\"Traduce la letra de la canci√≥n al espa√±ol si est√° en ingl√©s.\"\"\"\n",
    "    if pd.notna(text):  # Solo traducimos si hay letra disponible\n",
    "        return GoogleTranslator(source='en', target='es').translate(text)\n",
    "    return \"Traducci√≥n no disponible\"\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"Busca canciones en base a una frase del usuario, con c√°lculos optimizados.\"\"\"\n",
    "    \n",
    "    # üìå Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # üìå Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "    # üìå Calcular similitud en un solo paso (MUCHO M√ÅS R√ÅPIDO)\n",
    "    similarities = cosine_similarity(query_embedding, embedding_matrix)[0]\n",
    "\n",
    "    # üìå Agregar resultados a un nuevo dataframe\n",
    "    df['similarity'] = similarities\n",
    "\n",
    "    # üìå Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "\n",
    "    # üìå Traducir las letras al espa√±ol y agregar columna\n",
    "    top_songs['translated_lyrics'] = top_songs['processed_lyrics'].apply(translate_to_spanish)\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'translated_lyrics', 'similarity']]\n",
    "\n",
    "# üîç PRUEBA de velocidad con una consulta\n",
    "user_input = \"relojes derretidos en el tiemp\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original (Primeros 800 caracteres):**\\n\", lyrics[:800], \"...\")\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola (Primeros 800 caracteres):**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç PRUEBA de velocidad con una consulta\n",
    "user_input = \"Me encanta viajar sola\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original (Primeros 800 caracteres):**\\n\", lyrics[:800], \"...\")\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola (Primeros 800 caracteres):**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"sin m√∫sica la vida ser√≠a un error\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"He cometido el peor de los pecados que un hombre puede cometer: no he sido feliz\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elecci√≥n del Modelo Final\n",
    "Despu√©s de evaluar varios modelos, decidimos quedarnos con **all-roberta-large-v1** debido a su alta capacidad de comprensi√≥n sem√°ntica y su rendimiento en textos largos y detallados. Aunque es m√°s pesado y lento que otros modelos, su precisi√≥n y capacidad para capturar el contexto profundo de las letras de canciones lo hacen ideal para nuestro recomendador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusi√≥n\n",
    "El modelo **all-roberta-large-v1** ha demostrado ser el m√°s adecuado para nuestro sistema de recomendaci√≥n de canciones basado en letras y estados de √°nimo. Su capacidad para entender el contexto y la sem√°ntica de las frases completas nos permite ofrecer recomendaciones precisas y relevantes a los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementaci√≥n del Modelo Final\n",
    "En esta secci√≥n, implementamos el modelo **all-roberta-large-v1** para generar embeddings de las letras de canciones y realizar b√∫squedas sem√°nticas eficientes utilizando FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# üìå Rutas de archivos\n",
    "file_path = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\cleaned_songs_data_v2.csv\"\n",
    "faiss_index_path = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\lyrics_embeddings_faiss_IP.index\"\n",
    "\n",
    "# üìå Cargar dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# üìå Usar GPU si est√° disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('sentence-transformers/all-roberta-large-v1', device=device)  # üî• M√°xima calidad\n",
    "\n",
    "# üìå Crear texto combinado para embeddings (usamos `processed_lyrics`)\n",
    "df['combined_text'] = (\n",
    "    df['song_name'].astype(str) + \" \" +\n",
    "    df['artist_name'].astype(str) + \" \" +\n",
    "    df['processed_lyrics'].astype(str) + \" \" +\n",
    "    df['combined_genres'].astype(str) + \" \" +\n",
    "    df['playlists_names'].astype(str) + \" \" +\n",
    "    df['album_release_date'].astype(str) + \" \" +\n",
    "    df['language'].astype(str)\n",
    ")\n",
    "\n",
    "# üìå Par√°metros optimizados para evitar sobrecalentamiento\n",
    "batch_size = 2000  # üîπ Reducido de 5000 a 2000\n",
    "pause_time = 30  # ‚è∏Ô∏è Pausa de 30 segundos entre lotes\n",
    "total_rows = len(df)\n",
    "\n",
    "print(f\"üöÄ Total de canciones en el dataset: {total_rows}\")\n",
    "print(f\"üìå Procesando en bloques de {batch_size} canciones por batch con pausas de {pause_time}s.\")\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "# üìå Generar embeddings en bloques con pausas\n",
    "for start in range(0, total_rows, batch_size):\n",
    "    end = min(start + batch_size, total_rows)\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n‚ö° [Batch {start}-{end}] Iniciando procesamiento...\")\n",
    "\n",
    "    text_batch = df.iloc[start:end]['combined_text'].tolist()\n",
    "\n",
    "    if len(text_batch) == 0:\n",
    "        print(f\"‚ùå [Batch {start}-{end}] No hay datos para procesar. Saltando...\")\n",
    "        continue  \n",
    "\n",
    "    print(f\"‚è≥ [Batch {start}-{end}] Generando embeddings en GPU...\")\n",
    "\n",
    "    try:\n",
    "        embeddings_batch = model.encode(\n",
    "            text_batch,\n",
    "            batch_size=16,  # üîπ Se mantiene en 16 para no saturar GPU\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        embeddings_list.extend(embeddings_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en batch {start}-{end}: {str(e)}\")\n",
    "        continue  \n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚úÖ [Batch {start}-{end}] Embeddings generados en {elapsed_time:.2f} segundos.\\n\")\n",
    "\n",
    "    # ‚è∏Ô∏è Pausa para evitar sobrecalentamiento\n",
    "    print(f\"‚è≥ Esperando {pause_time}s para enfriar la CPU/GPU...\\n\")\n",
    "    time.sleep(pause_time)\n",
    "\n",
    "# üìå Convertir embeddings a NumPy y crear FAISS index\n",
    "print(\"üöÄ Guardando embeddings y optimizando FAISS...\")\n",
    "\n",
    "embeddings_np = np.array(embeddings_list, dtype=np.float32)\n",
    "d = embeddings_np.shape[1]\n",
    "\n",
    "# üìå Crear √≠ndice FAISS optimizado para b√∫squedas r√°pidas\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# üìå Guardar el √≠ndice FAISS optimizado\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "\n",
    "print(f\"‚úÖ FAISS `IndexFlatIP` guardado en {faiss_index_path}, listo para b√∫squedas sem√°nticas precisas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# üìå Ruta del archivo de embeddings generados con RoBERTa\n",
    "embeddings_file_roberta = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\embeddings_roberta2.pkl\"\n",
    "\n",
    "# üìå Cargar el dataset con los embeddings\n",
    "df = pd.read_pickle(embeddings_file_roberta)\n",
    "\n",
    "# üìå Cargar modelo RoBERTa\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Usando: {device.upper()}\")\n",
    "model = SentenceTransformer('all-roberta-large-v1', device=device)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Traduce la frase del usuario a ingl√©s si no est√° en ingl√©s.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"üåç Traducido '{text}' ‚ûù '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    Busca canciones en base a una frase de usuario.\n",
    "    \n",
    "    - user_query: Texto ingresado por el usuario (ejemplo: \"Me gusta conducir en invierno\").\n",
    "    - top_n: N√∫mero de canciones recomendadas.\n",
    "    \"\"\"\n",
    "    # üìå Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # üìå Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # üìå Asegurar que los embeddings en el DataFrame son listas\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: x if isinstance(x, list) else list(x))\n",
    "\n",
    "    # üìå Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([x], [query_embedding])[0][0])\n",
    "\n",
    "    # üìå Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'similarity']]\n",
    "\n",
    "# üîç Prueba con una consulta\n",
    "user_input = \"Comprad el libro de mi hermano, cabrones. Cuando sea famoso y tenga una serie en Netflix dir√©is que¬†lo¬†hab√©is¬†le√≠do\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados\n",
    "print(resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tarda to much :\n",
    "Vamos a probar con cosine_similarity() en un solo paso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "\n",
    "# üìå Ruta del archivo de embeddings generados con RoBERTa\n",
    "embeddings_file_roberta = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\embeddings_roberta2.pkl\"\n",
    "\n",
    "# üìå Cargar el dataset con los embeddings\n",
    "df = pd.read_pickle(embeddings_file_roberta)\n",
    "\n",
    "# üìå Convertir embeddings a NumPy arrays (para mejor rendimiento)\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "# üìå Cargar modelo RoBERTa en GPU si est√° disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚ö° Usando: {device.upper()}\")\n",
    "\n",
    "model = SentenceTransformer('all-roberta-large-v1', device=device)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Traduce la frase del usuario a ingl√©s si no est√° en ingl√©s.\"\"\"\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':\n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"üåç Traducido '{text}' ‚ûù '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "def translate_to_spanish(text):\n",
    "    \"\"\"Traduce una letra de ingl√©s a espa√±ol.\"\"\"\n",
    "    return GoogleTranslator(source='en', target='es').translate(text) if isinstance(text, str) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"\n",
    "    Busca canciones en base a una frase del usuario.\n",
    "    \n",
    "    - user_query: Texto ingresado por el usuario (ejemplo: \"Me gusta conducir en invierno\").\n",
    "    - top_n: N√∫mero de canciones recomendadas.\n",
    "    \"\"\"\n",
    "    # üìå Traducir la consulta si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "\n",
    "    # üìå Convertir la consulta en embedding\n",
    "    query_embedding = model.encode(translated_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    # üìå Calcular similitud con todas las canciones\n",
    "    df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity([x], [query_embedding])[0][0])\n",
    "\n",
    "    # üìå Ordenar por mayor similitud y devolver las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n).copy()\n",
    "\n",
    "    # üìå Traducir las letras de las canciones al espa√±ol\n",
    "    top_songs['translated_lyrics'] = top_songs['processed_lyrics'].apply(lambda x: translate_to_spanish(x[:500]) if isinstance(x, str) else \"Traducci√≥n no disponible\")\n",
    "\n",
    "    return top_songs[['artist_name', 'song_name', 'spotify_url', 'processed_lyrics', 'translated_lyrics', 'similarity']]\n",
    "\n",
    "# üîç Prueba con una consulta\n",
    "user_input = \"he pedido matrimonio a mi novia durante el rodaje de una pel√≠cula\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # üîπ Asegurar que las letras sean cadenas de texto v√°lidas\n",
    "    original_lyrics = str(row['processed_lyrics']) if isinstance(row['processed_lyrics'], str) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if isinstance(row['translated_lyrics'], str) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra en Ingl√©s:**\\n\", original_lyrics, \"...\")\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics)\n",
    "    print(\"=\"*500, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç PRUEBA de velocidad con una consulta\n",
    "user_input = \"Me encanta viajar sola\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # Asegurar que processed_lyrics no sea NaN y convertirlo en string\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original (Primeros 800 caracteres):**\\n\", lyrics[:800], \"...\")\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola (Primeros 800 caracteres):**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"sin m√∫sica la vida ser√≠a un error\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîç Prueba con una consulta en espa√±ol\n",
    "user_input = \"He cometido el peor de los pecados que un hombre puede cometer: no he sido feliz\" \n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Evitar errores con valores NaN\n",
    "    lyrics = str(row['processed_lyrics']) if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if pd.notna(row['translated_lyrics']) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", lyrics[:800], \"...\")  # Se imprimen los primeros 800 caracteres\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics[:800], \"...\")\n",
    "    print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Prueba con una consulta\n",
    "user_input = \"hard metal feliz\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados de forma clara\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify:** {row['spotify_url']}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "\n",
    "    # üîπ Asegurar que las letras sean cadenas de texto v√°lidas\n",
    "    original_lyrics = str(row['processed_lyrics']) if isinstance(row['processed_lyrics'], str) else \"Letra no disponible\"\n",
    "    translated_lyrics = str(row['translated_lyrics']) if isinstance(row['translated_lyrics'], str) else \"Traducci√≥n no disponible\"\n",
    "\n",
    "    print(\"\\nüìú **Letra Original:**\\n\", original_lyrics[:500], \"...\")\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola:**\\n\", translated_lyrics)\n",
    "    print(\"=\"*80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO BertLyrics - Este modelo intentamos usarlo pero estaba disponible y se vio que Roberta estaba reentrenado con √©l acerc√°ndose m√°s a nuestra finalidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "model_name = \"brunokreiner/lyrics-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Crear embedding para una frase de prueba\n",
    "sentence = \"I feel so lonely tonight\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener los embeddings de la √∫ltima capa\n",
    "embeddings = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Frase del usuario\n",
    "user_sentence = \"I feel so lonely tonight\"\n",
    "\n",
    "# Tokenizaci√≥n\n",
    "inputs = tokenizer(user_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Obtener embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraer el embedding del CLS token\n",
    "user_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "print(\"üìå Embedding generado:\", user_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# üìå Ruta del dataset\n",
    "file_path = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\final_df.csv\"\n",
    "\n",
    "# üìå Cargar el dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# üìå Seleccionar solo la columna de letras procesadas\n",
    "df = df[['recording_id', 'processed_lyrics']].dropna()\n",
    "\n",
    "# üìå Cargar el modelo de Hugging Face\n",
    "model_name = \"brunokreiner/lyrics-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# üìå Pasar el modelo a GPU si est√° disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# üìå Funci√≥n para generar embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# üìå Generar los embeddings para todas las canciones\n",
    "embeddings = []\n",
    "for lyrics in df['processed_lyrics']:\n",
    "    embedding = get_embedding(lyrics)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# üìå Convertir a numpy array y guardar\n",
    "embeddings_array = np.array(embeddings)\n",
    "np.save(\"lyrics_embeddings.npy\", embeddings_array)\n",
    "\n",
    "# üìå Guardar un dataframe con IDs y embeddings\n",
    "df_embeddings = pd.DataFrame({\n",
    "    'recording_id': df['recording_id'],\n",
    "    'embedding': list(embeddings)\n",
    "})\n",
    "df_embeddings.to_pickle(\"lyrics_embeddings.pkl\")\n",
    "\n",
    "print(\"‚úÖ ¬°Embeddings generados y guardados correctamente!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# üìå Rutas de los archivos\n",
    "embeddings_file = \"lyrics_embeddings.pkl\"  # Archivo con embeddings\n",
    "metadata_file = \"final_df.csv\"  # Archivo con datos completos de las canciones\n",
    "model_name = \"brunokreiner/lyrics-bert\"  # Modelo\n",
    "\n",
    "# üìå Cargar el dataset con embeddings\n",
    "df_embeddings = pd.read_pickle(embeddings_file)\n",
    "\n",
    "# üìå Cargar el dataset completo con metadatos (nombre de canci√≥n, artista, etc.)\n",
    "df_metadata = pd.read_csv(metadata_file)\n",
    "\n",
    "# üìå Fusionar embeddings con metadatos usando \"recording_id\"\n",
    "df = df_embeddings.merge(df_metadata, on=\"recording_id\", how=\"left\")\n",
    "\n",
    "# üìå Cargar modelo y tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# üìå Funci√≥n para traducir la frase del usuario a ingl√©s si es necesario\n",
    "def translate_to_english(text):\n",
    "    detected_lang = detect(text)\n",
    "    if detected_lang != 'en':  \n",
    "        translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        print(f\"üåç Traducido '{text}' ‚ûù '{translated_text}'\")\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "# üìå Funci√≥n para obtener el embedding de la frase del usuario\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# üìå Funci√≥n para traducir los primeros caracteres de la letra de la canci√≥n al espa√±ol\n",
    "def translate_lyrics_snippet(lyrics):\n",
    "    if pd.notna(lyrics) and isinstance(lyrics, str):\n",
    "        snippet = lyrics[:300]  # Primeros 300 caracteres\n",
    "        translated_snippet = GoogleTranslator(source='en', target='es').translate(snippet)\n",
    "        return translated_snippet\n",
    "    return \"Traducci√≥n no disponible\"\n",
    "\n",
    "# üìå Funci√≥n para buscar canciones seg√∫n la frase del usuario\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"Encuentra canciones en base a la frase ingresada por el usuario\"\"\"\n",
    "    \n",
    "    # üîé Traducir la frase si es necesario\n",
    "    translated_query = translate_to_english(user_query)\n",
    "    \n",
    "    # üîé Convertir la frase en embedding\n",
    "    query_embedding = get_embedding(translated_query).reshape(1, -1)\n",
    "    \n",
    "    # üìå Calcular similitudes con todas las canciones\n",
    "    song_embeddings = np.vstack(df['embedding'].values)  # Convertir embeddings a NumPy array\n",
    "    similarities = cosine_similarity(query_embedding, song_embeddings)[0]\n",
    "    \n",
    "    # üìå Agregar la similitud al dataframe\n",
    "    df['similarity'] = similarities\n",
    "    \n",
    "    # üìå Ordenar por mayor similitud y seleccionar las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "    \n",
    "    return top_songs[['recording_id', 'song_name', 'artist_name', 'spotify_url', 'processed_lyrics', 'similarity']]\n",
    "\n",
    "# üîç Prueba con una frase del usuario\n",
    "user_input = \"mis amigos son los mejores\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados en tu formato preferido\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify URL:** {row['spotify_url'] if pd.notna(row['spotify_url']) else 'Sin URL'}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Mostrar letra original (m√°ximo 800 caracteres)\n",
    "    lyrics = row['processed_lyrics'] if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    print(\"\\nüìú **Letra Original (Primeros 800 caracteres):**\")\n",
    "    print(lyrics[:800] + \"...\" if len(lyrics) > 800 else lyrics)\n",
    "\n",
    "    # Traducir los primeros 300 caracteres de la letra\n",
    "    translated_snippet = translate_lyrics_snippet(lyrics)\n",
    "    print(\"\\nüåç **Traducci√≥n Espa√±ola (Primeros 300 caracteres):**\")\n",
    "    print(translated_snippet)\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# üìå Rutas de los archivos\n",
    "embeddings_file = \"lyrics_embeddings.pkl\"  # Archivo con los embeddings de las canciones\n",
    "model_name = \"brunokreiner/lyrics-bert\"  # Modelo\n",
    "\n",
    "# üìå Cargar el dataset con los embeddings\n",
    "df = pd.read_pickle(embeddings_file)\n",
    "\n",
    "# üìå Cargar modelo y tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# üìå Funci√≥n para obtener el embedding de la frase del usuario\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# üìå Funci√≥n para buscar canciones seg√∫n la frase del usuario\n",
    "def search_songs(user_query, top_n=5):\n",
    "    \"\"\"Encuentra canciones en base a la frase ingresada por el usuario\"\"\"\n",
    "    \n",
    "    # üîé Convertir la frase en embedding\n",
    "    query_embedding = get_embedding(user_query).reshape(1, -1)\n",
    "    \n",
    "    # üìå Calcular similitudes con todas las canciones\n",
    "    song_embeddings = np.vstack(df['embedding'].values)  # Convertir embeddings a NumPy array\n",
    "    similarities = cosine_similarity(query_embedding, song_embeddings)[0]\n",
    "    \n",
    "    # üìå Agregar la similitud al dataframe\n",
    "    df['similarity'] = similarities\n",
    "    \n",
    "    # üìå Ordenar por mayor similitud y seleccionar las mejores coincidencias\n",
    "    top_songs = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "    \n",
    "    return top_songs[['recording_id', 'song_name', 'artist_name', 'spotify_url', 'processed_lyrics', 'similarity']]\n",
    "\n",
    "# üîç Prueba con una frase del usuario\n",
    "user_input = \"I feel broken and alone\"\n",
    "resultados = search_songs(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados en tu formato preferido\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify URL:** {row['spotify_url'] if pd.notna(row['spotify_url']) else 'Sin URL'}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Mostrar letra (m√°ximo 800 caracteres)\n",
    "    lyrics = row['processed_lyrics'] if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    print(\"\\nüìú **Letra Original (Primeros 800 caracteres):**\")\n",
    "    print(lyrics[:800] + \"...\" if len(lyrics) > 800 else lyrics)\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decarto este"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RED NEURONAL PARA JUNTAR EMBEDDINGS DE T5 - ROBERTA Y MPNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# üìå Cargar datasets con embeddings de diferentes modelos\n",
    "df_t5 = pd.read_pickle(\"clean_df_embeddings_t5.pkl\")\n",
    "df_roberta = pd.read_pickle(\"clean_df_embeddings_roberta.pkl\")\n",
    "df_mpnet = pd.read_pickle(\"clean_df_embeddings_768.pkl\")  # MPNet\n",
    "\n",
    "# üìå Unir los datasets usando recording_id como clave\n",
    "df = df_t5.merge(df_roberta, on=\"recording_id\", suffixes=(\"_t5\", \"_roberta\"))\n",
    "df = df.merge(df_mpnet, on=\"recording_id\")\n",
    "\n",
    "# üìå Convertir embeddings en arrays NumPy y normalizar\n",
    "def normalize_embedding(embedding_list):\n",
    "    embedding_array = np.array(embedding_list)\n",
    "    return embedding_array / np.linalg.norm(embedding_array)\n",
    "\n",
    "df[\"embedding_t5\"] = df[\"embedding_t5\"].apply(normalize_embedding)\n",
    "df[\"embedding_roberta\"] = df[\"embedding_roberta\"].apply(normalize_embedding)\n",
    "df[\"embedding_mpnet\"] = df[\"embedding\"].apply(normalize_embedding)  # MPNet\n",
    "\n",
    "# üìå Concatenar los embeddings en un solo vector\n",
    "df[\"combined_embedding\"] = df.apply(lambda row: np.concatenate([\n",
    "    row[\"embedding_t5\"],\n",
    "    row[\"embedding_roberta\"],\n",
    "    row[\"embedding_mpnet\"]\n",
    "]), axis=1)\n",
    "\n",
    "# üìå Guardar el dataset combinado\n",
    "df[[\"recording_id\", \"combined_embedding\"]].to_pickle(\"clean_df_embeddings_combined.pkl\")\n",
    "\n",
    "print(\"‚úÖ Embeddings fusionados y guardados correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# üìå Rutas de tus archivos\n",
    "csv_file = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df.csv\"\n",
    "embeddings_file = r\"C:\\Users\\solan\\Downloads\\get_data_from_songs\\src\\clean_df_embeddings_combined.pkl\"\n",
    "\n",
    "# üìå Cargar dataset\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# üìå Cargar embeddings\n",
    "df_embeddings = pd.read_pickle(embeddings_file)\n",
    "\n",
    "# üìå Unir ambos datasets por 'recording_id'\n",
    "df = df.merge(df_embeddings, on=\"recording_id\", how=\"inner\")\n",
    "\n",
    "# üìå Convertir embeddings a un array NumPy (FAISS necesita float32)\n",
    "embedding_matrix = np.vstack(df['combined_embedding'].values).astype(\"float32\")\n",
    "\n",
    "# üìå Crear √≠ndice FAISS\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])  # √çndice de similitud L2\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# üìå Cargar modelo para obtener embeddings de consulta\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "def get_combined_embedding(text):\n",
    "    \"\"\"Genera el embedding combinado usando los mismos modelos que se usaron en FAISS\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mpnet_output = mpnet_model(**inputs).last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "        t5_output = t5_model(**inputs).last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "        roberta_output = roberta_model(**inputs).last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "    # üìå Concatenar los embeddings en un solo vector de 2560 dimensiones (768+768+1024)\n",
    "    combined_embedding = np.concatenate([mpnet_output, t5_output, roberta_output])\n",
    "\n",
    "    return combined_embedding\n",
    "\n",
    "\n",
    "# üìå Funci√≥n para obtener embedding de la frase del usuario\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().astype(\"float32\")\n",
    "\n",
    "def search_songs_faiss(user_query, top_n=5):\n",
    "    query_embedding = get_combined_embedding(user_query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, top_n)  # FAISS realiza la b√∫squeda\n",
    "\n",
    "    # üìå Extraer resultados del dataframe\n",
    "    results = df.iloc[indices[0]].copy()\n",
    "    return results\n",
    "\n",
    "\n",
    "# üîç Prueba con una frase del usuario\n",
    "user_input = \"I feel broken and alone\"\n",
    "resultados = search_songs_faiss(user_input, top_n=5)\n",
    "\n",
    "# üìå Mostrar resultados en tu formato preferido\n",
    "for index, row in resultados.iterrows():\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéµ **Canci√≥n:** {row['song_name']}\")\n",
    "    print(f\"üé§ **Artista:** {row['artist_name']}\")\n",
    "    print(f\"üîó **Spotify URL:** {row['spotify_url'] if pd.notna(row['spotify_url']) else 'Sin URL'}\")\n",
    "    print(f\"‚úÖ **Similaridad:** {row['similarity']:.4f}\")\n",
    "    \n",
    "    # Mostrar letra (m√°ximo 800 caracteres)\n",
    "    lyrics = row['processed_lyrics'] if pd.notna(row['processed_lyrics']) else \"Letra no disponible\"\n",
    "    print(\"\\nüìú **Letra Original (Primeros 800 caracteres):**\")\n",
    "    print(lyrics[:800] + \"...\" if len(lyrics) > 800 else lyrics)\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Verificar dimensi√≥n del √≠ndice FAISS\n",
    "print(f\"FAISS Index Dim: {index.d}\")\n",
    "\n",
    "# üìå Generar el embedding de la consulta y verificar su dimensi√≥n\n",
    "query_embedding = get_embedding(\"I feel broken and alone\").reshape(1, -1)\n",
    "print(f\"Query Embedding Dim: {query_embedding.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_embeddings.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarda demasiado - pruebo faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripci√≥n estad√≠stica de las columnas num√©ricas\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
